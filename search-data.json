[
    
    
    
        
            {
                "id": 0,
                "href": "https://kihlander.net/post/extending-an-enum-in-zig/",
                "title": "Extending an Enum in Zig",
                "section": "post",
                "date" : "2024.09.01",
                "body": "For a while now I have been dabbling with zig and as a small test writing a NES-emulator (everyone should have written an 8-bit emulator in their life right?). While doing this I stumbled upon a kind of neat trick that you can do in zig that I thought was worth sharing\u0026hellip; with kind of a \u0026ldquo;anti-climactic\u0026rdquo; end!\nSome background While writing some debug-tools I decided that I wanted to switch between ppu-palettes when displaying some data. At this time the available palettes were all described with an enum.\npub const Palette = enum { bg0, // background palette 0 bg1, // background palette 1 bg2, // background palette 2 bg3, // background palette 3 sp0, // sprite palette 0 sp1, // sprite palette 1 sp2, // sprite palette 2 sp3, // sprite palette 3 }; As I use zig-gamedev for window-management, rendering and ImGui the first thing to reach for is just an imgui-combo-box. Zig:s builtin compile-time reflection and some smart decisions in the zig-gamedev imgui-bindings make that a real treat:\nconst zgui = @import(\u0026#34;zgui\u0026#34;); var Palette = .bg0; fn drawUi() { zgui.comboFromEnum(\u0026#34;palette\u0026#34;, \u0026amp;var); } This will create a combo-box with all possible values in the enum and select the names from the enum-item names.\nThe problem However, I wanted to have a \u0026ldquo;custom\u0026rdquo; selection as well? Zig has some really powerful meta-programming facilities, maybe we should try these out? I usually get a tingle of \u0026ldquo;bad things coming up\u0026rdquo; when I hear meta-programming as it brings back memories of slow compiles, hard to maintain code and c++-template-errors :D\nBut I\u0026rsquo;m here to learn, test and experiment, so here we go!\nLets see if we can extend Palette with a first option called custom, it is actually not that hard!\nfn addCustomChoice(comptime T: anytype) type { // we should probably add some error-checks here that T is an enum etc! // use reflection to get the information of the comptime parameter T const enum_type = @typeInfo(T).Enum; // define an array, in compile time, with the fields of our new enum // that has room for \u0026#34;custom\u0026#34;. comptime var fields: [enum_type.fields.len + 1]std.builtin.Type.EnumField = undefined; // define our first field to be \u0026#34;custom\u0026#34; and have the value \u0026#34;1 more than // the number of fields\u0026#34;. fields[0] = .{ .name = \u0026#34;custom\u0026#34;, .value = enum_type.fields.len }; // copy the field from the \u0026#34;base\u0026#34; enum inline for (1.., enum_type.fields) |idx, f| { fields[idx] = f; } // and declare and return our new type! const enumInfo = std.builtin.Type.Enum{ .tag_type = u8, .fields = \u0026amp;fields, .decls = \u0026amp;[0]std.builtin.Type.Declaration{}, .is_exhaustive = true, }; return @Type(std.builtin.Type{ .Enum = enumInfo }); } // define a new value of the new type and set it to our new value. var var_with_custom = addCustomChoice(Palette) = .custom; That is actually quite readable, it is just \u0026ldquo;ordinary code\u0026rdquo; that you can read as any other.\nBut there are some problems that I have to mention.\n\u0026ldquo;custom\u0026rdquo; is hardcoded and values are hardcoded. With this \u0026ldquo;custom\u0026rdquo; and the values of the new items are hardcoded but that could be solved by just taking that as a parameter or an array etc, nothing big!\ngetting the \u0026ldquo;base\u0026rdquo; enum back This is a bigger issue, that could probably be solved by someone better at zig than I am. The problem arises when I want to use my new enum as the \u0026ldquo;base\u0026rdquo; one, i.e. I want to pass what I selected in my combo-box to some other api. You can\u0026rsquo;t just pass your new enum to the other api as the types are not the same. My first thought was to just add a function to the enum like fn toBase(self: NewType) BaseType but I couldn\u0026rsquo;t find a good way through comptime type definitions to add a function to the declared type (probably missing something?).\nSo I just ended up with my own enumCast() declared like this:\nfn isEnum(comptime T: anytype) bool { return switch (@typeInfo(T)) { .Enum =\u0026gt; true, else =\u0026gt; false, }; } fn enumCast(in: anytype, comptime T: anytype) T { comptime { const in_T = @TypeOf(in); if (!isEnum(in_T)) @compileError(std.fmt.comptimePrint(\u0026#34;\u0026#39;in\u0026#39; should be an enum, \u0026#39;{s}\u0026#39; is not\u0026#34;, .{@typeName(in_T)})); if (!isEnum(T)) @compileError(std.fmt.comptimePrint(\u0026#34;\u0026#39;T\u0026#39; should be an enum, \u0026#39;{s}\u0026#39; is not\u0026#34;, .{@typeName(T)})); } const i = @intFromEnum(in); return @enumFromInt(i); } fn castMe() void{ callWithBase(enumCast(var_with_custom, Palette)); } I couldn\u0026rsquo;t find a \u0026ldquo;neater\u0026rdquo; way of casting between 2 enums in zig, should there be one? Maybe? Also having to pass in the type to \u0026ldquo;cast\u0026rdquo; to feels \u0026ldquo;old style zig\u0026rdquo; but there seems to not be a better way at the moment.\nthere is a proposal for @Return https://github.com/ziglang/zig/issues/447 that could be used here.\nThis is the point where I realized that I once again had been tricked by the temptress that is \u0026ldquo;meta programming\u0026rdquo;, what was I doing? Why all this code? And I went back to just doing this.\nvar var_with_custom : enum(u8) { custom = 8, bg0 = @intFromEnum(Palette.bg0), bg1 = @intFromEnum(Palette.bg1), bg2 = @intFromEnum(Palette.bg2), bg3 = @intFromEnum(Palette.bg3), sp0 = @intFromEnum(Palette.sp0), sp1 = @intFromEnum(Palette.sp1), sp2 = @intFromEnum(Palette.sp2), sp3 = @intFromEnum(Palette.sp3), fn asBase(self: @This()) Palette { if(self == .custom) unreachable; const b = @intFromEnum(self); return @enumFromInt(b); } } = .custom; Less code, easier to understand\u0026hellip; sure, someone might argue that this is not that reusable but honestly, the above wasn\u0026rsquo;t either. Some one else might argue \u0026ldquo;what if the \u0026lsquo;Palette\u0026rsquo; is extended?\u0026rdquo;\u0026hellip; I\u0026rsquo;ll take my chances that Nintendo will not redesign the NES any time soon!\nSo what have we learned by this? Number one is probably just because you can its not sure that you should! But also that zig:s meta-programming facilities are really powerful and easy to read/write. I really like the fact that \u0026ldquo;it is just plain zig\u0026rdquo; and that I don\u0026rsquo;t have to learn that much new stuff. I\u0026rsquo;m still afraid of the day when I have to debug this kind of stuff but I\u0026rsquo;ll take this over c++ templates any day of the week!\nIt is also obvious that zig has a long way to go\u0026hellip; documentation is \u0026ldquo;sparse\u0026rdquo; to say the least. The above code was all figured out with googling and reading the zig std-lib. On the other hand it is quite impressive that I could do this in a really short amount of time despite the lack of docs!\nIn the end it was an experiment that turned out to be a dead end\u0026hellip; but these are also worth writing about from time to time!\n"
            }
    
        ,
            {
                "id": 1,
                "href": "https://kihlander.net/post/compile-time-hashes-in-c-revisied/",
                "title": "Compile Time Hashes in C - Revisied",
                "section": "post",
                "date" : "2024.04.13",
                "body": "Hashing strings in c++ at compile time has been possible since c++11 but is it worth doing? Me and a few colleagues was discussed this over a few beers and it reminded me that I have already written about it here (8 years ago\u0026hellip; ARGH I\u0026rsquo;m getting old!).\nBut a lot of time has passed since I wrote that\u0026hellip; and I didn\u0026rsquo;t make any measurements in that article! shame! SHAME I SAY!\nSo it is time to revise this and answer some questions.\nWhat is the overhead of hashing strings at compile time) Are there other pros/cons doing it at compile-time compared to some preprocessor or script? Generating some test code To get some meaningful test-cases we probably need to test quite a lot of hashes and since I wasn\u0026rsquo;t really in the mood hand-write that I resorted to my trusty old friend python.\nAnd with the help of https://github.com/AntonJohansson/StaticMurmur and my own python lib https://github.com/wc-duck/pymmh3 it didn\u0026rsquo;t take long to whipp up a python script to generate cpp-file whit lots of hashes.\na note on the hash function used. I just picked MurmurHash3 as that is what I use at home and at work mostly\u0026hellip; is that a good one?\n¯\\(ツ)/¯\nAs good as any for this test I think!\nSo now we have this generated code:\n#include \u0026lt;stdint.h\u0026gt; #include \u0026#34;StaticMurmur.hpp\u0026#34; int switch_me(uint32_t val) { switch(val) { #if CONSTEXPR_HASH case murmur::static_hash_x86_32(\u0026#34;nlvykhgxncrkqjqg\u0026#34;, 0): return 0; case murmur::static_hash_x86_32(\u0026#34;jeqejajpfgbxadqq\u0026#34;, 0): return 1; case murmur::static_hash_x86_32(\u0026#34;psrgqorfrelbavmm\u0026#34;, 0): return 2; // ... lots of cases ... case murmur::static_hash_x86_32(\u0026#34;wddjastpsstmdizm\u0026#34;, 0): return 4095; #else case 0xc045b43c: return 0; case 0xecf91e72: return 1; case 0x8239d78b: return 2; // ... lots of cases ... case 0xcf83cfd8: return 4095; #endif default: break; } return 0; } A single switch with 4096 different values and no variation on the string length. I would guess that the length here is \u0026ldquo;around\u0026rdquo; the average string-length that at least I would expect to find being hash at compile-time (See, here I am not collecting the real data again!).\nGetting the numbers Since the script can generated a different amount of hashes I generated files with 16, 128, 1024, 2048 and 4096 hashes each and threw them at g++ and clang++ with both -O0 and -O2.\nCompilers used:\ng++ --version -\u0026gt; g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nclang++ --version -\u0026gt; Ubuntu clang version 14.0.0-1ubuntu1.1\n16 128 1024 2048 4096 g++ -O0 constant 0.015s 0.021s 0.078s 0.175s 0.475s g++ -O0 constexpr 0.015s 0.027s 0.140s 0.302s 0.750s clang++ -O0 constant 0.023s 0.028s 0.056s 0.084s 0.153s clang++ -O0 constexpr 0.026s 0.047s 0.203s 0.386s 0.750s g++ -O2 constant 0.017s 0.036s 0.304s 0.920s 4.041s g++ -O2 constexpr 0.018s 0.045s 0.371s 1.068s 4.370s clang++ -O2 constant 0.025s 0.039s 0.224s 0.612s 2.097s clang++ -O2 constexpr 0.030s 0.063s 0.392s 0.917s 2.716s If we look at the table we can clearly see that we pay something for using the constexpr hashes, it would be unexpected if we didn\u0026rsquo;t\u0026hellip; but it also do not seem to be that much. We can see that in the bigger numbers that the times is quite high regardless if just generating the hashes or using the constexpr hash. We clearly see that it take quite a bit of time to compile with both approaches.\nWhat is it that the compilers are spending their time on then? Optimizing the huge switch of course! Lets try again but this time we\u0026rsquo;ll just generate the constants!\nWhat the compilers do with the switch/case is out of scope for this post, maybe there will be a followup? Would be interesting to dig down into the generated assembly of that.\nWe\u0026rsquo;ll change the script to generate this instead.\n#include \u0026lt;stdint.h\u0026gt; #include \u0026#34;StaticMurmur.hpp\u0026#34; #if CONSTEXPR_HASH constexpr uint32_t const_0 = murmur::static_hash_x86_32(\u0026#34;trwlssxfykmuzljm\u0026#34;, 0); constexpr uint32_t const_1 = murmur::static_hash_x86_32(\u0026#34;wgyvldnumcqwvlmm\u0026#34;, 0); // ... lots of constants ... constexpr uint32_t const_15 = murmur::static_hash_x86_32(\u0026#34;vhjbpzwglrkisdvv\u0026#34;, 0); #else constexpr uint32_t const_0 = 0xc7f2b682; constexpr uint32_t const_1 = 0xade60248; // ... lots of constants ... constexpr uint32_t const_15 = 0xe4fa1635; #endif Compiling these in the same way gives us this:\n16 128 1024 2048 4096 g++ -O0 constant 0.011s 0.016s 0.024s 0.033s 0.053s g++ -O0 constexpr 0.012s 0.021s 0.093s 0.172s 0.313s clang++ -O0 constant 0.026s 0.025s 0.035s 0.048s 0.072s clang++ -O0 constexpr 0.037s 0.043s 0.129s 0.293s 0.533s g++ -O2 constant 0.011s 0.017s 0.021s 0.026s 0.038s g++ -O2 constexpr 0.012s 0.023s 0.089s 0.157s 0.305s clang++ -O2 constant 0.023s 0.025s 0.039s 0.050s 0.073s clang++ -O2 constexpr 0.029s 0.044s 0.142s 0.272s 0.517s Way faster!\nMost overhead seems to be in \u0026ldquo;optimizing the switch()\u0026rdquo;. Doing some quick math just dividing the diff between constant and constexpr we see that when the amount of hashes rise the time-per-hash flattens out to a constant, 0.06ms for gcc and 0.1ms on my machine. On the lower numbers I guess that the overhead on \u0026ldquo;everything except hashes\u0026rdquo; skews the numbers to make them unreliable.\nIt is kind of interesting that clang is that much slower than gcc but that just mean that there are room for improvements right? Is it worth it to compare with a compiled version of the hash-function? I did a really unscientific test with many factors that might skew the result gives us these numbers.\nTest app:\n#include \u0026lt;stdint.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026#34;StaticMurmur.hpp\u0026#34; const uint8_t* gen(uint32_t s) { srand(1337); uint8_t* buf = (uint8_t*)malloc(s); for(uint32_t i = 0; i \u0026lt; s; ++i) buf[i] = (uint8_t)rand(); return buf; } int main(int, const char**) { const uint8_t* buf = gen(1024 * 1024 * 128); uint32_t hash = murmur::MurmurHash3_x86_32((const char*)buf, s, 0); return hash; } That result in a speed of 0.8us per char while the gcc does it in compile time at a paltry 37.5us per char. Not a fair comparison at all, but worth putting in here just for visibility.\nConclusions So what have I learned? I don\u0026rsquo;t want to say \u0026lsquo;we\u0026rsquo; as you might come to other conclusions from these numbers than I did! Lets draw up some pros and cons to come to the conclusions.\npros of compile-time hashing:\nIt is easy to add hashes to your code. In theory some kind of pre-processor or script that I described in the previous post will always be the \u0026ldquo;fastest\u0026rdquo; solution to build. However from what I have seen in real code bases is that it is a significant hurdle to go and add a new .hash-file, or add your hash to a previous file. Maybe not so much a technical hurdle as a mental one.\nThis usually lead to code like this, something I am also guilty of:\n// TODO: move this to pre-compile step static const uint32_t my_hash = hash_string(\u0026#34;my string of doom!\u0026#34;); Surprise, surprise\u0026hellip; it will never be moved and it will just live there with all what that mean. It will never be expensive enough for anyone to fix but it will always be there and adding to your software being ever so slightly worse.\nWhat you also see in code using pre-compiled hash-files is that they are usually full of dead and unused hashes for everything and nothing. People have just added entries and when the code is removed they are forgotten in the source-files.\nThere is also the approach of having a custom preprocessor that runs over all your code before compile. I haven\u0026rsquo;t worked in a codebase that does that so take my opinion here with a great scoop of salt! I would guess that it will add some time as well and also add more complexity to build-pipelines etc. If you have a codebase where you can just throw all your files in a \u0026ldquo;compile all files at once, it compiles so fast anyways\u0026rdquo; it might be the best solution out there. But some of us just dream of that kind of luxury!\ncons of compile-time hashing:\nThe compiler can be quite finicky when it actually pre-compile constexpr hashes, at least in -O0 and it is kind of easy to get it evaluated at runtime (and one might actually argue that the compiler shouldn\u0026rsquo;t optimize this for debugging your functions!).\nCompare\n// evaluated in runtime in -O0 const uint32_t my_hash = my_constexpr_hash_string(\u0026#34;my string of doom!\u0026#34;); // evaluated in compile-time in -O0 constexpr uint32_t my_hash = my_constexpr_hash_string(\u0026#34;my string of doom!\u0026#34;); But to be honest, what I usually see, the static const uint32 my_hash = hash_string(\u0026quot;str\u0026quot;), will never evaluate in compile time either\u0026hellip; so that might be a moot point? It will result in worse code as c++ guarantees that my_hash is only initialized once. I.e. the compiler need to implement some kind of locking mechanism here. A quick godbolt shows us that yes, the compiler will generate an extra branch and a lock for the value, probably not that expensive as branch-prediction will almost always be a hit, but still.\nA bigger problem in my book might be that you actually do not have a central point where you can \u0026ldquo;collect\u0026rdquo; hashes for things such as checking for hash-collisions and setting up tools for hash to string lookups etc.\nFinal words After looking at the actual numbers and digging into this topic a bit deeper I might have changed position here, or if nothing else altered my view on the topic a bit. Doing hashing in compile-time is probably going to be fairly low-cost. How many strings like these do you actually have per file, probably not 4096 or more :)\nI would say that there is still use cases for a hash -\u0026gt; header generator or similar tool. If you have some code that goes into lots of other files it might be worth optimizing that case instead of everyone paying that cost all the time, and if you have some other kind of code-gen that generate hashes there is no reason at all why you wouldn\u0026rsquo;t pre-compute the hash outside of your c++ compilation.\nBut for \u0026ldquo;a few hashes here and there\u0026rdquo; you would probably be better of with the more user friendly option that make sure that it is actually used (and cleaned out when no longer in use!).\nBTW, I\u0026rsquo;m really annoyed that I didn\u0026rsquo;t look into the numbers \u0026ldquo;back then\u0026rdquo; as it would be interesting digging into how compilers has evolved on this topic over the years. Maybe this was way more expensive, finicky etc back then? I could dig up old compilers and test\u0026hellip; but no!\nAm I right? Am I wrong? Ping me in the usual channels if you have any comments!\nAppendix Here are the scripts used to generate the tests\u0026hellip; you can probably write them yourself in a short amount of time but here you are :)\ngen.py build.sh\n"
            }
    
        ,
            {
                "id": 2,
                "href": "https://kihlander.net/post/software-i-like/",
                "title": "Software I Like",
                "section": "post",
                "date" : "2024.03.10",
                "body": "I\u0026rsquo;m not the one that tinkers a lot with my dev-environment but there are some tools and software that I need and find that they help a lot day to day. And since I like to read about what others use I might as well share some tips on tools that you might also like.\nMy day-to-day OS of choice at home is xubuntu and at work Windows.\nI hope to make this post a living document so it might be updated from time to time.\nSome images in this post is stolen from the tools webpages, I really hope that that is not a problem, if it is, please reach out!\nSoftware clink - readline for the windows console The default terminal in Windos is damn near unusable, however thanks to Martin Ridgers wonderful little tool clink we can have a GNU readline based input in said terminal! This is a must-install on any windows machine that I do some work on!\nhttps://github.com/mridgers/clink/\nThe original repository has not been updated a quite a while and there is a fork here https://github.com/chrisant996/clink that claims to be maintaining the tool. I haven\u0026rsquo;t used that fork yet but I might give it a go some day.\nWinMerge - merging code Merging/diffing code is something that you do all of the time. My weapon of choice for a really long time has been winmerge. It does both code and directory diffs quite well and is the one I have stuck with and feel most comfortable with. However I can\u0026rsquo;t let go of the feeling that I should look for something new, for example WinMerge is windows only.\nMaybe I should look at writing something for myself? I would like something that both support windows and linux, that can be run in the console if I want to and has syntax highlighting.\nI\u0026rsquo;ll put that in the pile of \u0026lsquo;yet another project to start and maybe finish someday\u0026hellip; maybe\u0026hellip; hopefully\u0026hellip; probably not :D\u0026rsquo;\nWinDirStat - What is eating all my precious disk-space? At work I have to juggle quite a few different projects, all will a lot of GB:s of data and your bound to fill up your disk:s with temporary files, models, textures, object-files, and other build-artifacts.\nA great tool to visualize where you spend all these GB:s is WinDirStat. It is a great tool to give you an overview of where your disk is spent and make it simple to clean out what you don\u0026rsquo;t see and help you get an overview of where you can optimize!\nLately I have also started to test out WizTree that is faster than WinDirStat. I however hasn\u0026rsquo;t used it enough to say if I prefer it or not!\nEverything - Fast search in windows! What if windows search didn\u0026rsquo;t suck? In comes Everything! Lighting fast \u0026ldquo;find me the file with this name on my machine\u0026rdquo;, can\u0026rsquo;t live without it after starting to use it.\nI am looking for an alternative to use at home on linux but haven\u0026rsquo;t found anything yet but FSearch might be worth testing out?\nImHex - A great hex-editor. From time to time you need a hex-editor and I have found ImHex to be a great alternative. No installation, fast and snappy, feature-rich and no fuzz\u0026hellip; exactly like a tool should be!\nbat - A better cat. There is always times when you just want to check the content of a file without editing etc. The usual solution on a unix based system is to reach for cat. bat is basically cat with syntax highlighting!\nReally nice when you just want to check out some code and I have used it together with objdump for better assembly output!\nobjdump -C -d file | bat -l asm\nGet it here\nPython Python need no introduction, but it is my weapon of choice for \u0026ldquo;hacking together a small script\u0026rdquo; or just as a command line tool for doing quick math or converting utf8 strings to hex-bytes etc.\nWould I like to write a bigger program in python? NEVER AGAIN :)\nVisual Studio Plugins Visual Studio is what I usually write code in professionally for better and worse. I use it kind of vanilla, but there are some plugins that make it a better experience.\nSmart Command Line Arguments I work with a lot of command-line argument heavy applications so juggling command-line arguments to turn on and off features or run this or that unittest etc can get kind of daunting with the default command line interface in Visual Studio.\nSmart Command Line Arguments helps out a lot and make that work a lot smoother.\nA word of warning, it will override all settings done in the ordinary vs-interface and that can bite you in the behind!\nCompile Score A word of warning, this plugin can become kind of an addiction!\nCompile Score is a really neat plugin that hook into your build without any extra code or setup, works just as well with your custom buildsystem as with ordinary msbuild, and show you in a great way where you spend time compiling and what includes cost you the most etc.\nTry it out but don\u0026rsquo;t blame me when your hooked :)\nOn the \u0026ldquo;radar\u0026rdquo; These are tools that I haven\u0026rsquo;t tried yet, isn\u0026rsquo;t released yet etc but that I has some hope for.\nRAD Debugger RAD Debugger on GitHub\nThe world need some competition in the debugger-space and as a Linux user I definitively know!!! Professionally I use Visual Studio but it has been getting slower and slower and more bloated over time. So when Epic/RAD anounced that they are working on their own debugger and released a preview build that really made me want to try it out. Unfortunatly I haven\u0026rsquo;t had the time yet, but it is there to keep an eye on!\nDisk Voyager Disc Voyager is an in development replacement for windows explorer that looks really neat! Seems fast an well thought out. I would live to give it a spin and test it out some day.\n"
            }
    
        ,
            {
                "id": 3,
                "href": "https://kihlander.net/post/looking-at-c-for-better-closures-in-cpp/",
                "title": "Looking at c for better closures in c++",
                "section": "post",
                "date" : "2024.01.07",
                "body": " Note: before we begin, finding a name for this post was really hard!\nIn this post I\u0026rsquo;m going to touch on a c++-technique to handle callbacks that I have not seen written about before and that many of my colleagues hadn\u0026rsquo;t seen before either. Probably it\u0026rsquo;s not something new and some of you will probably just say \u0026ldquo;yeah yeah, nothing new under the sun\u0026rdquo; but it\u0026rsquo;s probably worth a few words!\nMost of us has been in situations where we need to pass a function + userdata to another function. It might be that we have some kind of \u0026ldquo;for each\u0026rdquo; over some collection of things or polling events from a system.\nI, for example, like to write systems that need polling that can also emit events/results that happened since the last poll for the user to react to. One convenient way to do this is to just pass a callback to your poll-function that is called per item. This will not \u0026ldquo;force\u0026rdquo; any storage on the user such as returning an allocated array would do and leave the actual decision on what to do with the data to the user.\nSomething like this:\nstruct the_system_msg { msg_type type; // ... payload goes here! ... union { struct event1 { // ... payload if type == event1 }; struct event2 { // ... payload if type == event2 }; } evt; }; void the_system_poll(the_system sys, /*callback here*/); void poll_me(the_system sys) { some_other_system\u0026amp; other = get_other_system(); // our poll-function will give you a callback for x amount of events... you don\u0026#39;t know how many... but \u0026#34;a potential bunch\u0026#34;. the_system_poll(sys, [\u0026amp;other](const the_system_msg\u0026amp; msg) { switch(msg.type) { case type_event1: other.do_stuff_with_x(msg.event1); break; case type_event2: other.do_stuff_with_y(msg.event2); break; } }); } As you can see I left out \u0026lsquo;callback\u0026rsquo; here as that is what we are about to come to now.\nstd::function The canonical way of doing this in c++ is to reach for std::function, something like this:\n#include \u0026lt;functional\u0026gt; void the_system_poll(the_system sys, std::function\u0026lt;void(const the_system_msg\u0026amp; msg)\u0026gt; cb); This works\u0026hellip; but it is not without its drawbacks!\nMemory allocations First of, std::function can allocate memory on the heap, something that would be wasteful if we aren\u0026rsquo;t saving our closure, i.e. the lifetime of the closure is the same as function-call. All modern std::-lib implementations seem to optimize that by putting smaller closures into the std::function object itself, but bigger ones is forced to end up on the heap.\nThis behavior can lead to your application all of a sudden starting to allocate without you seeing it. For example you might need to \u0026ldquo;just capture one more int\u0026rdquo; or a struct \u0026ldquo;grows\u0026rdquo; without you seeing it. Boom, allocation creeping in!\nIs an allocation here and there a massive problem? Probably not for code like this BUT, at least in my line of work, trying to not do work that you don\u0026rsquo;t have to is \u0026ldquo;in the job description\u0026rdquo; and one day that thing you were calling 3 times a frame is now being called all over the place.\nCompile times Including \u0026lt;functional\u0026gt; on my system adds a shitton (metric!) of lines to compile to you pre-processed c++-file! However you turn this, throwing more code on your compiler to work with will probably not make it complete faster! We will get to numbers and comparisons later on!\nDebug performance Thirdly, debug-performance! Yes, we should care about performance in debug-builds as optimized builds can be way harder to use when tracking down issues.\nThe fact that, at work, I can run a full debug-build of Apex (the Avalanche Engine) and still reach decent performance is worth a lot to your day to day productivity!\nAs we can see in a previous post about swapping memory we can see that the c++ standard library can be far from great in a non optimized build (and honestly in an optimized one as well)!\nJust pass the closure! So, what can we do instead? \u0026lsquo;Just pass the closure\u0026rsquo; is the simplest solution! This is suggested by many, for example in this article.\nSomething like this:\ntemplate\u0026lt;typename FUNC\u0026gt; void the_system_poll(the_system sys, FUNC\u0026amp;\u0026amp; cb) { // ... implement me ... } Yay, no more std::function! This should solve all the lines included from \u0026lt;functional\u0026gt; and will probably make your perf in a -O0 build quite a bit better! Yet again we will come to numbers later!\nSo all numbers look great (trust me!), we are all happy right? RIGHT? Well not quite. What does the above code really mean? It means that all our code in the_system_poll() need to be inlined due to the template. For a smaller function this is just fine and maybe even desired! But in this case it might mean that we need to inline a big part of a bigger system! What if the_system need a lot of lines of code to implement or that the implementation of the storage for the_system requires a whole bunch of expensive includes to just be able to be declared. We would not want to expose that to your humble user just by including the_system.h!\nHeader hygiene is a virtue!\nc-style So how do we handle this? As usual a good way to solve this is to look at a c-style interface. This is something I personally see as the solution to many problems and maybe a topic for its own post some day :)\nWhen I say c-style here doesn\u0026rsquo;t necessarily mean full c, just functions + handles + structs. A reference here and a constexpr there is just fine.\nBut how would this look if you would do it in c? Probably something like this:\nvoid the_system_poll(the_system sys, void(*cb)(the_system_msg\u0026amp; msg, void* userdata), void* userdata); I.e. we would pass a function pointer and userdata as a void* and on the implementation side cast that void* back to what we originally passed in. By doing this we can put the entire implementation of this function in a .c/.cpp-file and hide it for the user! This works but the ergonomics maybe leave a bit to be desired:\n// ... first we need to declare our payload sturct to pass to our callback ... struct my_user_data { int data1; int data2; }; // ... and then declare the actuall callback function ... static void poll_function(const the_system_msg\u0026amp; msg, void* user_data) { // ... cast our passed in userdata to our payload struct ... my_user_data* ud = (my_user_data*)user_data; // ... run the actual code that we want to run ... use_me(ud-\u0026gt;data1); use_me(ud-\u0026gt;data2); } void poll_me(the_system sys) { // ... declare our payload sturct and fill out the data we need to use ... my_user_data ud; ud.data1 = some_value1; ud.data1 = some_value2; // ... and call it ... the_system_poll(sys, poll_function, \u0026amp;ud); } That is quite a bit of code and honestly quite a few things to get wrong.\nKihlanders reverse But what if we combine these 2 approaches? I.e. use the classical c-style function + userdata to be able to hide away all implementation and use the templated closure for ergonomics! If we combine them it could look something like this:\nvoid the_system_poll(the_system sys, void(*cb)(the_system_msg\u0026amp; msg, void* userdata), void* userdata); template\u0026lt;typename FUNC\u0026gt; void the_system_poll(the_system sys, FUNC\u0026amp;\u0026amp; cb) { // ... add a second wrapper-function to handle the casting for us ... auto wrap = [](const the_system_msg\u0026amp; msg, void* userdata) { // ... as we passed a pointer to the closure as userdata // we can just cast it back here and call it and we get // all the payload etc for free ... FUNC\u0026amp; f = *(FUNC*)userdata; f(msg); }; // ... pass the wrapper as the callback to the c-function and the // generated closure as userdata ... the_system_poll(sys, wrap, \u0026amp;cb); } If this hasn\u0026rsquo;t been described before I would like to dub this Kihlanders reverse, it has a nice ring to it right?\nThis will make it possible to write this:\nvoid poll_me(the_system sys) { the_system_poll(sys, [\u0026amp;some_value1, \u0026amp;some_value2]()){ use_me(some_value1); use_me(some_value2); }); } Just by introducing a 5 line wrapper we can give the user all the ergonomics of the original std::function without much of the cost! We also have an API that is compatible with c and all the languages that can call c by just adding a pre-processor check for __cplusplus:\n#if defined(__cplusplus) // .... #endif Numbers! But enough talk about perf without numbers!\nBefore we start, this is the standard disclaimer about micro benchmarks. They are hard and might be inaccurate compared to a real world scenario etc. You know the drill!\nAll this work has been done on my laptop with the following specs:\nCPU Intel i7-10710U\nRAM 16GB LPDDR3 at 2133 MT/s (around 17GB/sec peak bandwidth)\nAnd I\u0026rsquo;ll use the compilers that I have installed, that being\nClang: 14.0.0-1ubuntu1.1\nGCC: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nCompile time First of, lets look at the compile-time claim. Let\u0026rsquo;s create the smallest possible test-file that we can create:\n#if defined(USE_STD_FUNC) #include \u0026lt;functional\u0026gt; #endif struct the_system_msg { int i; }; #if defined(USE_STD_FUNC) void func(int sys, std::function\u0026lt;void(const the_system_msg\u0026amp;)\u0026gt; cb); #else void func(int sys, void(*)(const the_system_msg\u0026amp; msg, void* userdata), void* user_data); template\u0026lt;typename FUNC\u0026gt; void func(int sys, FUNC\u0026amp;\u0026amp; cb) { auto wrap = [](const the_system_msg\u0026amp; msg, void* userdata) { FUNC\u0026amp; f = *(FUNC*)userdata; f(msg); }; func(sys, wrap, \u0026amp;cb); } #endif Timing this small file is probably not that realistic, but lets do it anyways and see what we end up with. We can start with noticing that the time is basically the same across all the optimization levels (-O0, -O2, -O3, -Os). Not really surprising as we don\u0026rsquo;t give the compiler anything to work with\u0026hellip;\nBut just using time we get roughly these numbers for clang and gcc.\nwc-duck@WcLaptop:~/kod/functor_test$ time clang++ -c -O2 functor_preproc.cpp -o functor_preproc.o real 0m0,031s user 0m0,009s sys 0m0,023s wc-duck@WcLaptop:~/kod/functor_test$ time clang++ -c -O2 -D USE_STD_FUNC functor_preproc.cpp -o functor_preproc.o real 0m0,105s user 0m0,084s sys\t0m0,021s wc-duck@WcLaptop:~/kod/functor_test$ time g++ -c -O2 functor_preproc.cpp -o functor_preproc.o real 0m0,022s user 0m0,009s sys 0m0,008s wc-duck@WcLaptop:~/kod/functor_test$ time g++ -c -O2 -D USE_STD_FUNC functor_preproc.cpp -o functor_preproc.o real 0m0,174s user 0m0,137s sys 0m0,032s This is highly unscientific, but we see a diff in cost just compiling the code and in a bigger codebase things like this tend to add up. But where is the time spent. We could dig in deeper with something like clang -ftime-report but it is probably enough to just look at the pre-processed code.\nPreprocessed code for the non-std::function code is about the same lines that we wrote, i.e.\n# 1 \u0026#34;functor_preproc.cpp\u0026#34; # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 1 # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 3 # 404 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 3 # 1 \u0026#34;\u0026lt;command line\u0026gt;\u0026#34; 1 # 1 \u0026#34;\u0026lt;built-in\u0026gt;\u0026#34; 2 # 1 \u0026#34;functor_preproc.cpp\u0026#34; 2 struct the_system_msg { int i; }; void func(int sys, void(*)(const the_system_msg\u0026amp; msg, void* userdata), void* user_data); template\u0026lt;typename FUNC\u0026gt; void func(int sys, FUNC\u0026amp;\u0026amp; cb) { auto wrap = [](const the_system_msg\u0026amp; msg, void* userdata) { FUNC\u0026amp; f = *(FUNC*)userdata; f(msg); }; func(sys, wrap, \u0026amp;cb); } I\u0026rsquo;m pretty sure that you don\u0026rsquo;t want me to paste out the thousands of lines that you get with std::function. Depending on what c++-version you target you get these numbers, these are lines with all empty lines stripped via:\ng++ -E functor_preproc.cpp -DUSE_STD_FUNC -std=c++98 | sed \u0026#39;/^\\s*#/d;/^\\s*$/d\u0026#39; | wc -l It is worth noting that I decided to strip out empty lines as the preprocessor seem to produce a lot of it. My really uneducated guess is that it is just faster for the preprocessor to strip out \u0026ldquo;ifdef\u0026rdquo;:ed code by switching the lines with new-lines instead of removing them from the data properly? But that is just a guess. However I think it is much fairer to count the lines without the empty lines as a compiler probably handle these lines quickly.\nHowever, here are the line counts for a few different c++ versions.\nstd=c++98 std=c++11 std=c++14 std=c++17 std=c++20 clang 505 8477 9252 23622 27211 gcc 505 8477 9248 23589 27180 That is a lot of lines compared to 14 that was the non std-one! Regardless of how you put it, that will take time to process. And this is BEFORE we have actually started to turn all these lines into instructions for the CPU to execute!\nPerformance Next up is performance, how do the different solutions stand up against each other? To test this out we\u0026rsquo;ll write a benchmark app using the excellent ubench.h.\nfunctor_bench.cpp\nI have added a few different test-cases to benchmark, both tested with a \u0026lsquo;small\u0026rsquo; capture and \u0026lsquo;big\u0026rsquo; one where the \u0026lsquo;big\u0026rsquo; one should be big enough to not trigger small-object optimization. All \u0026ldquo;calling back into user code\u0026rdquo; 1000-times per iteration.\nstd::function passed to a non-inlined function std::function passed to an inlined function just pass a simple closure to an inlined function a c-style function passing a void* userdata and a Kihlanders reverse one. Let\u0026rsquo;s see how they perform, these are the times captured by the benchmark.\ngcc -O0 gcc -O2 clang -O0 clang -O2 std::function small 32.5us 1.3us 24.2us 1.5us std::function big 29.2us 1.3us 23.2us 1.5us inlined std::function small 32.9us 1.5us 24.2us 1.5us inlined std::function big 27.6us 1.5us 23.2us 1.8us inlined closure small 3.2us 0.02us 2.9us 0.02us inlined closure big 3.2us 0.02us 2.9us 0.02us c-style small 5.9us 1.3us 4.6us 1.4us c-style big 5.9us 1.3us 4.4us 1.4us Kihlanders reverse small 8.3us 1.3us 5.3us 1.4us Kihlanders reverse big 8.3us 1.3us 5.4us 1.4us these are mean values of the above tests. To note is also that the results are not super stable but \u0026lsquo;within reason\u0026rsquo; determined by me.\nSo what can we take away from these numbers.\nOne takeaway is, just as in swapping memory-article, debug-performance of std:: is just horrible. You are paying a lot for that \u0026ldquo;convenience\u0026rdquo; in your non-optimized build. I find it really \u0026ldquo;amusing\u0026rdquo; that the small-object version is actually significantly slower than the bigger closure in debug on gcc!\nSecondly an inline function is, obviously, faster in all builds and in -O2 it is taken as far as both gcc and clang just calculating the answer to my benchmark right away and just store an int directly. Interestingly they can\u0026rsquo;t do the same thing with the std::function version even if you inline it. Throwing complexity at the compiler will force the compiler to spend time on the complexity instead of optimizing the actual code!\nI also find it interesting that clang seems to do quite a bit better work with all of the code in -O0 while gcc performs better in -O2.\nFinally we see that Kihlanders reverse seem to add no overhead to any of the non-inlined alternatives and generally better than std::function in all cases.\nConclusion According to me this is a really nifty way to provide your users with a good API at low cost in both compile-time and performance. I, for example, use this in https://github.com/wc-duck/dirutil and it\u0026rsquo;s dir_walk() function and in quite a few API:s in the \u0026ldquo;Apex Engine\u0026rdquo;. It is obviously not for all your usecases as you can\u0026rsquo;t store the closure and using something like this for a sort or similar that you want inlined is probably not a good idea.\nSo to close this article, was this useful? Did I miss something? Feel free to reach out and tell me (if you are civil that is :) ).\n"
            }
    
        ,
            {
                "id": 4,
                "href": "https://kihlander.net/post/swapping-memory-and-compiler-optimizations/",
                "title": "Swapping memory and compiler optimizations",
                "section": "post",
                "date" : "2023.03.11",
                "body": "During my vacation for the holidays I thought that maybe I wanted some smaller project that you could fit in together with \u0026ldquo;family life\u0026rdquo; (not the easiest of endeavour!) and I got to think about some old code that I had laying about in my own little game-engine that I have thought about making public for a while. I thought it might be useful for someone else and maybe just doing some optimization work on it might be a fun little distraction!\nNow about 1.5 years later, I didn\u0026rsquo;t say what holidays right, The post is finally done!\nThe work on this has been a constant cycle of:\n\u0026lsquo;Why this difference?\u0026rsquo; Ahh\u0026hellip; ok! Take a few weeks without looking at it\u0026hellip; \u0026lsquo;Where were I? Ahhh soon done\u0026hellip; I just have this to check!\u0026rsquo; \u0026lsquo;WTF? why did this happen?\u0026rsquo; Rinse and repeat! But here we are, finally!\nmemcpy_util.h That code was a small header called memcpy_util.h containing functions to work on memory buffers, operations such as copy, swap, rotate, flip etc.\nSaid and done, I did set out to work on breaking it out of my own codebase, updating docs, fixing some API:s, adding a few more unittests and putting some benchmarks around the code as prep for having a go at optimizing the functions at hand.\nKudos to Scott Vokes for greatest.h and Neil Henning for the excellent little ubench.h!.\nThe code is published on github at the same time as this post goes live. However some quite interesting things popped out while benchmarking the code. I would say that most of this is not rocket-surgery and many of you might not find something new in here. But what the heck, the worst thing that can happen is that someone comes around and tell me that I have done some obvious errors and I\u0026rsquo;ll learn something, otherwise maybe someone else might learn a thing or two?\nIt should also be noted that what started as a single article might actually turn out to be a series, we\u0026rsquo;ll see what happens :)\nBefore we start I would like to acknowledge that I understand that writing compilers is hard and that there are a bazillion things to consider when doing so. This is only observations and not me bashing \u0026ldquo;them pesky compiler-developers\u0026rdquo;!\nPrerequisites All this work has been done on my laptop with the following specs:\nCPU Intel i7-10710U\nRAM 16GB LPDDR3 at 2133 MT/s (around 17GB/sec peak bandwidth)\nAnd I\u0026rsquo;ll use the compilers that I have installed, that being\nClang: 10.0.0-4ubuntu1\nGCC: g++ (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\nAnd all the usual caveats on micro-benchmarking goes here as well!\nSwapping memory buffers So we\u0026rsquo;ll start where I started, by swapping memory in 2 buffers, something that is the basis of many of the other operations in memcpy_util.h, such as flipping an image. What I thought would be a quick introduction turned out to be the entire article, so lets get to swapping memory between 2 buffers!\nThe first thing to notice is that c do not have a memswap(). c++ do have std::swap_ranges() but we\u0026rsquo;ll get back to that later!\nHowever, just implementing your own memswap() is a simple operation as long as you do not want to get fancy. I would consider this the simplest thing you could do!\nNote: I am not handling overlap of the buffers to swap in this version as that was not something that was currently needed. Probably there should be an assert() or similar that checks for overlap however.\nGeneric memswap inline void memswap( void* ptr1, void* ptr2, size_t bytes ) { uint8_t* s1 = (uint8_t*)ptr1; uint8_t* s2 = (uint8_t*)ptr2; for( size_t i = 0; i \u0026lt; bytes; ++i ) { uint8_t tmp = s1[i]; s1[i] = s2[i]; s2[i] = tmp; } } How does such a simple function perform? It turns out \u0026ldquo;it depends\u0026rdquo; is the best answer to that question!\nTo answer the question we\u0026rsquo;ll add 2 benchmark functions, one to swap a really small buffer and one to swap a quite large one (4MB).\nUBENCH_NOINLINE void memswap_noinline(void* ptr1, void* ptr2, size_t s) { memswap(ptr1, ptr2, s); } UBENCH_EX(swap, small) { uint8_t b1[16]; uint8_t b2[16]; fill_with_random_data(b1); fill_with_random_data(b2); UBENCH_DO_BENCHMARK() { memswap_noinline(b1, b2, sizeof(b1)); } } UBENCH_EX(swap, big) { const size_t BUF_SZ = 4 * 1024 * 1024; uint8_t* b1 = alloc_random_buffer\u0026lt;uint8_t\u0026gt;(BUF_SZ); uint8_t* b2 = alloc_random_buffer\u0026lt;uint8_t\u0026gt;(BUF_SZ); UBENCH_DO_BENCHMARK() { memswap_noinline(b1, b2, BUF_SZ); } free(b1); free(b2); } Notice how memswap() was wrapped in a function marked as noinline, this as clang would just optimize the function away otherwise.\nTime to take a look at the results, we\u0026rsquo;ll look at perf at different optimization level (perf in debug/-O0 is also important!) as well as generated code-size.\nAll graphs in this article is generated by my own tool that I don\u0026rsquo;t recommend any one to use, but here is a link anyways! wccharts\nthe variance on these are quite high, so these numbers is me \u0026lsquo;getting feeling\u0026rsquo; and guessing at a mean :)\nDebug - -O0 Lets start with -O0 and just conclude that both clang and gcc generates basically the same code as would be expected. There is nothing magic here (and nor should there be in a debug-build!) and the code performs there after. A simple for-loop that swaps values as stated in the code.\nlooking at the generated assembly\nAgain most readers might be familiar with this but checking the generated asm on unix:es is easily done with \u0026lsquo;objdump\u0026rsquo;\nobjdump -C -d local/linux_x86_64/gcc/O2/memcpy_util_bench | less\nor using the excellent little tool bat to get some nice syntax-highlighting\nobjdump -C -d local/linux_x86_64/gcc/O2/memcpy_util_bench | bat -l asm\nclang -O0\n\u0026lt;memswap_generic(void*, void*, unsigned long)\u0026gt;: push %rbp mov %rsp,%rbp mov %rdi,-0x8(%rbp) mov %rsi,-0x10(%rbp) mov %rdx,-0x18(%rbp) mov -0x8(%rbp),%rax mov %rax,-0x20(%rbp) mov -0x10(%rbp),%rax mov %rax,-0x28(%rbp) movq $0x0,-0x30(%rbp) mov -0x30(%rbp),%rax cmp -0x18(%rbp),%rax jae 418c2b \u0026lt;memswap_generic(void*, void*, unsigned long)+0x7b\u0026gt; mov -0x20(%rbp),%rax mov -0x30(%rbp),%rcx mov (%rax,%rcx,1),%dl mov %dl,-0x31(%rbp) mov -0x28(%rbp),%rax mov -0x30(%rbp),%rcx mov (%rax,%rcx,1),%dl mov -0x20(%rbp),%rax mov -0x30(%rbp),%rcx mov %dl,(%rax,%rcx,1) mov -0x31(%rbp),%dl mov -0x28(%rbp),%rax mov -0x30(%rbp),%rcx mov %dl,(%rax,%rcx,1) mov -0x30(%rbp),%rax add $0x1,%rax mov %rax,-0x30(%rbp) jmpq 418bd8 \u0026lt;memswap_generic(void*, void*, unsigned long)+0x28\u0026gt; pop %rbp retq nopl (%rax) gcc -O0\n\u0026lt;memswap_generic(void*, void*, unsigned long)\u0026gt;: endbr64 push %rbp mov %rsp,%rbp mov %rdi,-0x28(%rbp) mov %rsi,-0x30(%rbp) mov %rdx,-0x38(%rbp) mov -0x28(%rbp),%rax mov %rax,-0x10(%rbp) mov -0x30(%rbp),%rax mov %rax,-0x8(%rbp) movq $0x0,-0x18(%rbp) mov -0x18(%rbp),%rax cmp -0x38(%rbp),%rax jae 62bd \u0026lt;memswap_generic(void*, void*, unsigned long)+0x7a\u0026gt; mov -0x10(%rbp),%rdx mov -0x18(%rbp),%rax add %rdx,%rax movzbl (%rax),%eax mov %al,-0x19(%rbp) mov -0x8(%rbp),%rdx mov -0x18(%rbp),%rax add %rdx,%rax mov -0x10(%rbp),%rcx mov -0x18(%rbp),%rdx add %rcx,%rdx movzbl (%rax),%eax mov %al,(%rdx) mov -0x8(%rbp),%rdx mov -0x18(%rbp),%rax add %rax,%rdx movzbl -0x19(%rbp),%eax mov %al,(%rdx) addq $0x1,-0x18(%rbp) jmp 626f \u0026lt;memswap_generic(void*, void*, unsigned long)+0x2c\u0026gt; nop pop %rbp retq Optimized - -O2/-O3 At -O2 we will see that clang finds that it can use the SSE-registers to copy the data and gives us a huge speedup at the cost of roughly 2.5x the code size. Huge in this case is 9600 us vs 310 us, i.e. near 31 times faster!\nIf we look at the generated assembly we can see that the meat-and-potatoes of this function just falls down to copying the data with SSE vector-registers + a preamble that handles all bytes that are not an even multiple of 16, i.e. can\u0026rsquo;t be handled by the vector registers.\nListing the assembly generated here would be quite verbose, but the main loop doing the heavy lifting looks like this:\nclang -O2/-O3\n# ... # 401aa0 movups (%rdi,%rcx,1),%xmm0 movups 0x10(%rdi,%rcx,1),%xmm1 movups (%rsi,%rcx,1),%xmm2 movups 0x10(%rsi,%rcx,1),%xmm3 movups %xmm2,(%rdi,%rcx,1) movups %xmm3,0x10(%rdi,%rcx,1) movups %xmm0,(%rsi,%rcx,1) movups %xmm1,0x10(%rsi,%rcx,1) movups 0x20(%rdi,%rcx,1),%xmm0 movups 0x30(%rdi,%rcx,1),%xmm1 movups 0x20(%rsi,%rcx,1),%xmm2 movups 0x30(%rsi,%rcx,1),%xmm3 movups %xmm2,0x20(%rdi,%rcx,1) movups %xmm3,0x30(%rdi,%rcx,1) movups %xmm0,0x20(%rsi,%rcx,1) movups %xmm1,0x30(%rsi,%rcx,1) add $0x40,%rcx add $0xfffffffffffffffe,%r9 jne 401aa0 \u0026lt;memswap_generic_noinline(void*, void*, unsigned long)+0xb0\u0026gt; # ... This code is fast! And if I were to guess there is some heuristic in clang that detects the pattern of swapping memory buffers and have a fast-path for it and that we are not seeing any \u0026ldquo;clever\u0026rdquo; auto-vectorization (said by \u0026ldquo;not an expert (tm)\u0026rdquo;). If I\u0026rsquo;m wrong I would love to hear about it so that I can make a clarification here!\nWe can also observe that clang generates identical code with -O3, this is something that will show up consistently through out this article.\nNow lets look at gcc as that is way more interesting.\ngcc -O2\n\u0026lt;memswap_generic_noinline(void*, void*, unsigned long)\u0026gt;: endbr64 test %rdx,%rdx je 3ae9 \u0026lt;memswap_generic_noinline(void*, void*, unsigned long)+0x29\u0026gt; xor %eax,%eax nopl 0x0(%rax,%rax,1) movzbl (%rdi,%rax,1),%ecx movzbl (%rsi,%rax,1),%r8d mov %r8b,(%rdi,%rax,1) mov %cl,(%rsi,%rax,1) add $0x1,%rax cmp %rax,%rdx jne 3ad0 \u0026lt;memswap_generic_noinline(void*, void*, unsigned long)+0x10\u0026gt; retq nopw 0x0(%rax,%rax,1) First of we see that gcc generates really small code with -02, just 42 bytes. Code that is way slower than clang but still a great improvement over the non optimized code, 9600 us vs 2450 us, nearly 4 times faster. It has just generated a really simple loop and removed some general debug-overhead (such as keeping bytes in its own register and loading/storing it).\nIn -O3 however\u0026hellip; now we reach the same perf as clang in -02, but with double the code size, what is going on here? Well, loop-unrolling :)\nusing compiler explorer we can see that msvc is generating similar code as gcc for /O2 and also quite similar code for /O3, i.e. loop-unrolling!\nSmall - -Os In -Os both clang and gcc generate almost identical code, and that is code very close to what gcc generates in -02. Really small, efficient \u0026ldquo;enough\u0026rdquo;\u0026hellip; ok I guess.\nUse memcpy() in chunks So what can we do to generate better code on gcc in -O2? How about we try to just change the copy to use memcpy() instead? I.e. using memcpy() in chunks of, lets say 256 bytes? This should hopefully also improve our perf in -O0\nSomething like:\ninline void memswap_memcpy( void* ptr1, void* ptr2, size_t bytes ) { uint8_t* s1 = (uint8_t*)ptr1; uint8_t* s2 = (uint8_t*)ptr2; char tmp[256]; size_t chunks = bytes / sizeof(tmp); for(size_t i = 0; i \u0026lt; chunks; ++i) { size_t offset = i * sizeof(tmp); memcpy(tmp, s1 + offset, sizeof(tmp)); memcpy(s1 + offset, s2 + offset, sizeof(tmp)); memcpy(s2 + offset, tmp, sizeof(tmp)); } memcpy(tmp, s1 + chunks * sizeof(tmp), bytes % sizeof(tmp) ); memcpy(s1 + chunks * sizeof(tmp), s2 + chunks * sizeof(tmp), bytes % sizeof(tmp) ); memcpy(s2 + chunks * sizeof(tmp), tmp, bytes % sizeof(tmp) ); } First, lets compare with the generic implementation. \u0026hellip; and lets just look at the memcpy-versions by them self. Now this is better! Both for clang ang gcc we are outperforming the \u0026lsquo;generic\u0026rsquo; implementation by a huge margin in debug and we see clang being close to the same perf as -O2/-O3 in debug!:\nDebug perf\ngeneric GB/sec memcpy GB/sec perf clang 9600 us 0.4 370 us 10.6 26x gcc 9600 us 0.4 525 us 7.4 18x There are a few things that we might want to dig into here!\nWhy is clang this much faster than gcc? As we can see, clang is generating faster code in all configs, and usually smaller as well. The only exception when it comes to size is -Os where gcc generate really small code.\nBut what make the code generated by clang faster? Let\u0026rsquo;s start with a look at -O0 and the disassembly of the generated code.\nThe actual assembly can be found in the appendix (clang, gcc) as the listing is quite big.\nI feel like I\u0026rsquo;m missing something here\u0026hellip; is GCC inlining the copy for x bytes and if the copy is bigger it falls back to memcpy? Need to understand this better.\nLooking at the disassembly we can see that gcc has decided to replace many of the calls to memcpy() (however not all of them?) with a whole bunch of unrolled \u0026lsquo;mov\u0026rsquo; instructions while clang has decided to still generate calls to memcpy().\nUnfortunately for gcc this inlined code is a lot slower than the system memcpy() implementation. That kind of makes sense that calling into an optimized memcpy() from debug code would yield faster execution when copying larger chunks of memory. I would guess that gcc has tried to optimized for the case where the memcpy() would be small and the jump to memcpy would eat all perf-gain? I don\u0026rsquo;t know what heuristics went into this but I\u0026rsquo;ll ascribe it to \u0026ldquo;it is hard to write a compiler and what is best for x is not necessarily best for y\u0026rdquo;.\nOne thing we can try is to get gcc to call memcpy() by calling it via a pointer and by that not inline it. Something like this?\ninline void memswap_memcpy( void* ptr1, void* ptr2, size_t bytes ) { void* (*memcpy_ptr)(void*, const void*, size_t s) = memcpy; uint8_t* s1 = (uint8_t*)ptr1; uint8_t* s2 = (uint8_t*)ptr2; char tmp[256]; size_t chunks = bytes / sizeof(tmp); for(size_t i = 0; i \u0026lt; chunks; ++i) { size_t offset = i * sizeof(tmp); memcpy_ptr(tmp, s1 + offset, sizeof(tmp)); memcpy_ptr(s1 + offset, s2 + offset, sizeof(tmp)); memcpy_ptr(s2 + offset, tmp, sizeof(tmp)); } memcpy_ptr(tmp, s1 + chunks * sizeof(tmp), bytes % sizeof(tmp) ); memcpy_ptr(s1 + chunks * sizeof(tmp), s2 + chunks * sizeof(tmp), bytes % sizeof(tmp) ); memcpy_ptr(s2 + chunks * sizeof(tmp), tmp, bytes % sizeof(tmp) ); } First observation, clang generate the same code for all configs except -O0.\nSecondly, in -O0, we see WAY better perf on gcc and slightly better on clang. Calling into an optimized memcpy(), albeit via a pointer, instead of a bunch of unrolled mov instructions seem like a smart thing to do :)\nNext up, lets have a look at -O2/-O3, here we see that clang still decide to just call memcpy() and be done with it while gcc tries to be smart and add an inlined vectorized implementation using the SSE-registers (this is the same vectorization that it uses when its just a pure memcpy()). Unfortunately for GCC it\u0026rsquo;s generated memcpy-replacement is both slower and bulkier than just calling memcpy() directly resulting in both slower and bigger code :(\nAn interesting observation here is that in the measurements we see that clang is faster when going through a function pointer than directly calling memcpy(). I found this quite odd and checked the generated assembly\u0026hellip; and that is identical! As I wrote earlier, all the usual caveats on micro benchmarking apply :D\nmemcpy(), to inline or not to inline, thats the question? Calling memcpy or inlining, how do the compiler decide if it should call the system memcpy() or generate its own? It is not really clear to me and it would be interesting to dig into but I feel that it is out of the scope of this already big post. Maybe there will be a follow up some day :)\nManual vectorization with SSE Next up\u0026hellip; we found on the generic implementations that clangs vectorization performed quite well\u0026hellip; and what the compiler can do we can do as well right?\ninline void memswap_sse2( void* ptr1, void* ptr2, size_t bytes ) { size_t chunks = bytes / sizeof(__m128); // swap as much as possible with the sse-registers ... for(size_t i = 0; i \u0026lt; chunks; ++i) { float* src1 = (float*)ptr1 + i * (sizeof(__m128) / sizeof(float)); float* src2 = (float*)ptr2 + i * (sizeof(__m128) / sizeof(float)); __m128 tmp =_mm_loadu_ps(src1); _mm_storeu_ps(src1, _mm_loadu_ps(src2)); _mm_storeu_ps(src2, tmp); } // ... and swap the remaining bytes with the generic swap ... uint8_t* s1 = (uint8_t*)ptr1 + chunks * sizeof(__m128); uint8_t* s2 = (uint8_t*)ptr2 + chunks * sizeof(__m128); memswap_generic(s1, s2, bytes % sizeof(__m128)); } Again lets compare with the generic implementation. \u0026hellip; and the sse2-versions among them selfs. Data in table form\ngeneric GB/sec memcpy GB/sec sse2 GB/sec clang -O0 9600 us 0.4 370 us 10.56 1540 us 2.54 clang -Os 2900 us 1.35 270 us 14.47 280 us 13.95 clang -O2 310 us 12.6 318 us 12.28 310 us 12.60 clang -O3 307 us 12.6 318 us 12.28 310 us 12.60 gcc -O0 9600 us 0.4 525 us 7.44 1550 us 2.54 gcc -Os 2900 us 1.35 570 us 6.85 285 us 13.71 gcc -O2 2450 us 1.59 355 us 11.0 320 us 12.21 gcc -O3 325 us 12.0 355 us 11.0 320 us 12.21 Now we\u0026rsquo;r talking. By sacrificing support on all platforms and only focusing on x86 we can get both compilers to generate code that can compete with the calls to memcpy() in all but the -O0 builds. IMHO that is not surprising as we are comparing an optimized memcpy() against unoptimized code, however 1.5ms compared to the generic implementations 9.6ms is nothing to scoff at!\nFor better perf it seems it might be worth calling the memcpy-version in debug, but should one select different code-paths depending on optimization level\u0026hellip; not really sure? Maybe hide it behind a define and let the user decide?\nOr why not just make a pure assembly implementation right away?\nManual vectorization with AVX So if going wide with SSE registers was this kind of improvement, will it perform even better if we go wider with AVX? Lets try it out!\ninline void memswap_avx( void* ptr1, void* ptr2, size_t bytes ) { size_t chunks = bytes / sizeof(__m256); // swap as much as possible with the avx-registers ... for(size_t i = 0; i \u0026lt; chunks; ++i) { float* src1 = (float*)ptr1 + i * (sizeof(__m256) / sizeof(float)); float* src2 = (float*)ptr2 + i * (sizeof(__m256) / sizeof(float)); __m256 tmp = _mm256_loadu_ps(src1); _mm256_storeu_ps(src1, _mm256_loadu_ps(src2)); _mm256_storeu_ps(src2, tmp); } // ... and swap the remaining bytes with the generic swap ... uint8_t* s1 = (uint8_t*)ptr1 + chunks * sizeof(__m256); uint8_t* s2 = (uint8_t*)ptr2 + chunks * sizeof(__m256); memswap_generic(s1, s2, bytes % sizeof(__m256)); } Avx vs SSE2 They seem fairly similar in perf even as the AVX implementation is consistently slightly faster in optimized builds. Clang generally performing a bit better than gcc perf-wise. However the most interesting thing is seeing that clang in -O0 makes such a poor job of AVX compared to SSE while gcc seems to handle it just fine, actually generating faster -O0-code than the SSE-versions.\nWe\u0026rsquo;ll get to that later, but first \u0026hellip;\nUnrolling! Another thing we found when looking at clangs generated SSE-code was that it was unrolled to do 4 swaps each iteration of the loop. Will that bring us better perf in our sse and avx implementations? lets try!\nsize_t chunks = bytes / sizeof(__m128); for(size_t i = 0; i \u0026lt; chunks / 4; ++i) { float* src1_0 = (float*)ptr1 + (i + 0) * (sizeof(__m128) / sizeof(float)); float* src1_1 = (float*)ptr1 + (i + 1) * (sizeof(__m128) / sizeof(float)); float* src1_2 = (float*)ptr1 + (i + 2) * (sizeof(__m128) / sizeof(float)); float* src1_3 = (float*)ptr1 + (i + 3) * (sizeof(__m128) / sizeof(float)); float* src2_0 = (float*)ptr2 + (i + 0) * (sizeof(__m128) / sizeof(float)); float* src2_1 = (float*)ptr2 + (i + 1) * (sizeof(__m128) / sizeof(float)); float* src2_2 = (float*)ptr2 + (i + 2) * (sizeof(__m128) / sizeof(float)); float* src2_3 = (float*)ptr2 + (i + 3) * (sizeof(__m128) / sizeof(float)); __m128 tmp0 = _mm_loadu_ps(src1_0); __m128 tmp1 = _mm_loadu_ps(src1_1); __m128 tmp2 = _mm_loadu_ps(src1_2); __m128 tmp3 = _mm_loadu_ps(src1_3); _mm_storeu_ps(src1_0, _mm_loadu_ps(src2_0)); _mm_storeu_ps(src1_1, _mm_loadu_ps(src2_1)); _mm_storeu_ps(src1_2, _mm_loadu_ps(src2_2)); _mm_storeu_ps(src1_3, _mm_loadu_ps(src2_3)); _mm_storeu_ps(src2_0, tmp0); _mm_storeu_ps(src2_1, tmp1); _mm_storeu_ps(src2_2, tmp2); _mm_storeu_ps(src2_3, tmp3); } memswap_sse2((float*)ptr1 + chunks * (sizeof(__m128) / sizeof(float)), (float*)ptr2 + chunks * (sizeof(__m128) / sizeof(float)), bytes - chunks * sizeof(__m128)); First of, it seems that we gain a bit of perf yes, nothing major but still nothing to scoff at! However what I find mostly interesting is how, in -O0, clang generate similar code as gcc for SSE, but way worse for AVX, same as for the non-unrolled case? What\u0026rsquo;s going on here?\nIf we inspec the generated assembly for the sse-version, both clang and gcc has generated almost the same code. There is an instruction here and there that are a bit different, but generally the same.\nHowever for AVX the story is different\u0026hellip;\nassembly can be found in the appendix, (clang, gcc)\nfirst of, if we look at code size, we see that clang has generate a function clocking in at 1817 byte while gcc is clocking in at 1125 bytes. All of this diff in size is taken up by the fact tha gcc has decided to use vinsertf128 and vextractf128 while clang decide to do the same move to and from registers with your plain old mov and quite a few of them.\nI guess gcc has just been coming further in their AVX support than clang. This is not my field of expertise, so I might have missed something crucial here. If I have, please point it out!\nHow about std::swap_ranges() and std::swap()? Now I guess some of you ask yourself, why doesn\u0026rsquo;t he just use what is given to him by the c++ standard library? It is after all \u0026ldquo;standard\u0026rdquo; and available to all by default, it should be at least decent right? So let\u0026rsquo;s add some benchmarks and just test it out! According to all info I can find std::swap_ranges() is the way to go.\nSo lets add the benchmark, run and\u0026hellip; OH NO!\nOn my machine, with -O0, it runs in about 3.3x the time on clang and 4.7x slower on gcc than the generic version we started of with! And compared to the fastest ones that we have implemented ourself its almost 112x slower in debug! Even if we don\u0026rsquo;t \u0026ldquo;cheat\u0026rdquo; and call into an optimized memcpy() we can quite easily device a version that run around 32x faster!\nEven the optimized builds only reach the same perf as we do with the standard \u0026lsquo;generic\u0026rsquo; implementation we had to begin with, not to weird as if you look at its implementation it is basically a really complex way of writing what we had in the generic case!\nI\u0026rsquo;m leaving comparing compile-time of \u0026ldquo;generic loop\u0026rdquo; vs \u0026ldquo;memcpy_util\u0026rdquo; vs \u0026ldquo;std::swap_ranges()\u0026rdquo; as an exercise for the reader!\nSo lets dig into why the performance is so terrible in debug for std::swap_ranges\u0026hellip; should we maybe blame the \u0026ldquo;lazy compiler devs\u0026rdquo;? Nah, not really, the compiler is really just doing what it was told to do, and it was told to generate a lot of function-calls!\nLets take a trip to compiler explorer and have a look at what assembly is actually generated for this.\nstd::swap_ranges()\nswap_it(): push rbp mov rbp, rsp mov eax, OFFSET FLAT:b1+4096 mov edx, OFFSET FLAT:b2 mov rsi, rax mov edi, OFFSET FLAT:b1 call unsigned char* std::swap_ranges\u0026lt;unsigned char*, unsigned char*\u0026gt;(unsigned char*, unsigned char*, unsigned char*) nop pop rbp ret Only 15 lines of assembly\u0026hellip; nothing really interesting here, we\u0026rsquo;ll have to dig deeper. Time to tell compiler explorer to show \u0026ldquo;library functions\u0026rdquo;\nstd::swap_ranges() - expanded\nunsigned char* std::swap_ranges\u0026lt;unsigned char*, unsigned char*\u0026gt;(unsigned char*, unsigned char*, unsigned char*): push rbp mov rbp, rsp sub rsp, 32 mov QWORD PTR [rbp-8], rdi mov QWORD PTR [rbp-16], rsi mov QWORD PTR [rbp-24], rdx .L3: mov rax, QWORD PTR [rbp-8] cmp rax, QWORD PTR [rbp-16] je .L2 mov rdx, QWORD PTR [rbp-24] mov rax, QWORD PTR [rbp-8] mov rsi, rdx mov rdi, rax call void std::iter_swap\u0026lt;unsigned char*, unsigned char*\u0026gt;(unsigned char*, unsigned char*) add QWORD PTR [rbp-8], 1 add QWORD PTR [rbp-24], 1 jmp .L3 .L2: mov rax, QWORD PTR [rbp-24] leave ret swap_it(): push rbp mov rbp, rsp mov eax, OFFSET FLAT:b1+4096 mov edx, OFFSET FLAT:b2 mov rsi, rax mov edi, OFFSET FLAT:b1 call unsigned char* std::swap_ranges\u0026lt;unsigned char*, unsigned char*\u0026gt;(unsigned char*, unsigned char*, unsigned char*) nop pop rbp ret void std::iter_swap\u0026lt;unsigned char*, unsigned char*\u0026gt;(unsigned char*, unsigned char*): push rbp mov rbp, rsp sub rsp, 16 mov QWORD PTR [rbp-8], rdi mov QWORD PTR [rbp-16], rsi mov rdx, QWORD PTR [rbp-16] mov rax, QWORD PTR [rbp-8] mov rsi, rdx mov rdi, rax call std::enable_if\u0026lt;std::__and_\u0026lt;std::__not_\u0026lt;std::__is_tuple_like\u0026lt;unsigned char\u0026gt; \u0026gt;, std::is_move_constructible\u0026lt;unsigned char\u0026gt;, std::is_move_assignable\u0026lt;unsigned char\u0026gt; \u0026gt;::value, void\u0026gt;::type std::swap\u0026lt;unsigned char\u0026gt;(unsigned char\u0026amp;, unsigned char\u0026amp;) nop leave ret std::enable_if\u0026lt;std::__and_\u0026lt;std::__not_\u0026lt;std::__is_tuple_like\u0026lt;unsigned char\u0026gt; \u0026gt;, std::is_move_constructible\u0026lt;unsigned char\u0026gt;, std::is_move_assignable\u0026lt;unsigned char\u0026gt; \u0026gt;::value, void\u0026gt;::type std::swap\u0026lt;unsigned char\u0026gt;(unsigned char\u0026amp;, unsigned char\u0026amp;): push rbp mov rbp, rsp sub rsp, 32 mov QWORD PTR [rbp-24], rdi mov QWORD PTR [rbp-32], rsi mov rax, QWORD PTR [rbp-24] mov rdi, rax call std::remove_reference\u0026lt;unsigned char\u0026amp;\u0026gt;::type\u0026amp;\u0026amp; std::move\u0026lt;unsigned char\u0026amp;\u0026gt;(unsigned char\u0026amp;) movzx eax, BYTE PTR [rax] mov BYTE PTR [rbp-1], al mov rax, QWORD PTR [rbp-32] mov rdi, rax call std::remove_reference\u0026lt;unsigned char\u0026amp;\u0026gt;::type\u0026amp;\u0026amp; std::move\u0026lt;unsigned char\u0026amp;\u0026gt;(unsigned char\u0026amp;) movzx edx, BYTE PTR [rax] mov rax, QWORD PTR [rbp-24] mov BYTE PTR [rax], dl lea rax, [rbp-1] mov rdi, rax call std::remove_reference\u0026lt;unsigned char\u0026amp;\u0026gt;::type\u0026amp;\u0026amp; std::move\u0026lt;unsigned char\u0026amp;\u0026gt;(unsigned char\u0026amp;) movzx edx, BYTE PTR [rax] mov rax, QWORD PTR [rbp-32] mov BYTE PTR [rax], dl nop leave ret std::remove_reference\u0026lt;unsigned char\u0026amp;\u0026gt;::type\u0026amp;\u0026amp; std::move\u0026lt;unsigned char\u0026amp;\u0026gt;(unsigned char\u0026amp;): push rbp mov rbp, rsp mov QWORD PTR [rbp-8], rdi mov rax, QWORD PTR [rbp-8] pop rbp ret Ouch\u0026hellip; we have call instructions generated for std::remove_reference, std::enable_if and std::iter_swap (so much for zero-cost abstractions)\u0026hellip; and there is nothing wrong with that from a compiler standpoint, you told it that you had functions that needed to be called so the compiler will generate the functions call! FYI the same code is generated for std::swap, std::array and similar constructs as well.\nWhy did the code end up like this? I can\u0026rsquo;t really answer that as I have neither written or, with an emphasis on, maintained a standard library implementation. I see that the generic code that we have there today lead to less code to maintain and maybe the concept of swapping buffers of memcpy:able data is not something that std::swap_ranges isn\u0026rsquo;t often used for but there is absolutely room for improvement here.\nJust having a top-level check for \u0026ldquo;can be moved via memcpy()\u0026rdquo; and have a plain for-loop in that case would generate faster code in debug-builds for all of us. But as stated, I have not worked on a standard library implementation nor have I maintained one (a \u0026ldquo;standard library\u0026rdquo; for a commercial, AAA game-engine however!) so I\u0026rsquo;m sure that I have missed lots of details here ¯\\(ツ)/¯\nWhat rubs me the wrong way with this is that there is nothing in the spec of std::swap_ranges that say that it has to be implemented(!) generically for all underlying types. If the type can be moved with a memcpy it could be implemented by a simple loop (or even better something optimized!).\nThis was done in Apex:s containers recently and saved us a BUNCH of perf, especially in debug-builds but also in optimized builds!\nThis is code and APIs used by millions of developers around the world, all of them having less of a chance to use a debug-build to track down their hairy bugs and issues. I can see the logic behind \u0026ldquo;just have one implementation for all cases\u0026rdquo; and how that might make sense if you look at code from a \u0026ldquo;purity\u0026rdquo; standpoint but in this case there are such a huge amount of developers that are affected that imho that \u0026ldquo;purity\u0026rdquo; is not important at all. Your assignment as standard library developers should not be to write readable and \u0026ldquo;nice\u0026rdquo; code (or maybe it is and in that case that is not the right focus!) it is to write something that work well for all the developers using your code! And that goes for non-optimized builds as well!\nWe have only tested on 4MB, how do we fare on smaller and bigger buffers? Up until now we have only checked performance on 4MB buffers but what happen in smaller and bigger buffers? Lets add some tests over a range of buffer sizes, we\u0026rsquo;ll add benchmarks on swapping buffers from 16 bytes up to 2GB in \u0026ldquo;reasonable\u0026rdquo; intervals and plot them against each other as a byte/sec vs buffer-size. For comparison we\u0026rsquo;ll also add in a pure memcpy() perf test as a benchmark as well.\nWe\u0026rsquo;ll split this section in debug and release build so let the graph-fest begin!\nDifferent buffer-sizes, -O0 Fist of, some graphs for all our implementations.\nIt is clear that we have a point ( around 12MB ) where perf flattens out for all implementations and everything above that it not really interesting, this for both compilers. It just happens to be that my machine has an L3 cache size of 12 MB! Another observation is that after this point the best implementation of memswap is roughly half the perf of a pure memcpy which also make sense as that is the mount of memory that has to be copied in a swap :D\nSo conclusion 1, our best implementation in debug (just using memcpy()) saturates the memory-bus! Hard to beat that!\nWe also see that all other implementations seem to be CPU-bound on different levels as they basically perform the same on all buffer-sizes and is NOT reaching what the memory on the machine should support.\nCpu-cache sizes can be queried on via cat /sys/devices/system/cpu/cpu0/cache/index[0-3]/size\nLets remove memcpy()-only and zoom in on sizes under 12MB!\nIt is clear than calling into memcpy is performing well on both compilers. It is also clear here that, as we concluded earlier, that GCC:s inlining of memcpy() is not doing it any favours here as going via a pointer (i.e. tricking gcc not to inline) is so much faster!\nWe also see how much better gcc performs with AVX than clang, we have dug into this before but these graphs shows it clearly.\nDo we have to mention the \u0026ldquo;flat-line\u0026rdquo; that is std::swap_ranges()?\nOther than this, the both compilers do a similar job an all implementations!\nDifferent buffer-sizes, -O2/ -O3 Lets dig into the optimized builds!\nFirst thing we can see here is that over 12MB all implementations just flattens out at \u0026ldquo;memory speed\u0026rdquo; except for the generic implementation and std::swap_ranges() that on GCC performs really bad while on clang the same as the others.\nSo lets focus on what happens under the L3-cache boundary and remove the pure-memcpy()!\nA short note on code size A short note on code size as we haven\u0026rsquo;t really dug into it yet. From my point of view code-size of this code is not really interesting. Back in the old days of the PS3 and SPU:s it definitively was, but today I think there is bigger fish to fry. At least for code like this that tend to only be called in a few spots. However if it would be a problem a simple fix would be to just not inline the code as is done now. I doubt that on the kind of buffer-sizes where this would be used that extra call overhead would make any difference what so ever.\nHowever for other sectors of this business I guess it could be of a lot of importance.\nSo just for completeness, lets have a quick look at code size of the different investigated implementations.\nAt the time of writing I do not have access to a windows-machine for me to test out msvc on but I will add a few observations on generated code fetched via compiler explorer but no numbers.\ndumping function size\nFor most readers this is nothing new, but dumping symbol/function-sizes is easily done on most unix:es with the use of \u0026rsquo;nm\u0026rsquo;.\nnm --print-size -C local/linux_x86_64/clang/O2/memcpy_util_bench | grep memswap\nstd::swap_ranges in -O0 is an estimate and sum of all non-inlined std functions, functions used are\nWhat is most interesting to note is that GCC is, in most cases, generating much smaller code and seem to optimize for that a lot harder. Could that be due to gcc being used more in software where that is more desireable? I can only guess and it surely seems like it.\nIt would be interesting to hear if there is someone with more knowledge about this than me :)\nSummary So what have we learned? Honestly I\u0026rsquo;m not really sure :) Writing compilers that generate good code is hard? I feel that both clang and gcc do a decent job at what is presented to them, of course there are more things you can do if you know your problem up front compared to producing an optimized result from \u0026ldquo;whatever the user throws your way\u0026rdquo;.\nWe can also conclude that clang seems to do a better job when it comes to perf compared to GCC, that on the other hand generate smaller code on average. This being true with one caveat, that GCC does a much better job with AVX than clang.\nThis might seem like ordinary bashing of c++ standard libraries and I really didn\u0026rsquo;t want it to be\u0026hellip; but debug perf is important! Compile time is important! and it seems like it isn\u0026rsquo;t really taken into account when it should be. I\u0026rsquo;m not alone in seeing this. In the last year we have seen other more \u0026ldquo;c++-leaning\u0026rdquo; developers also raising this issue. For example Vittorio Romeo has been raising that std::move, std::forward and other \u0026ldquo;small\u0026rdquo; function generate expensive calls in debug that really isn\u0026rsquo;t needed and has been pushing for changes to both clang and gcc + it now seems that msvc is also coming along for the ride with [[msvc::intrinsic]]\nSee: and:\nthe sad state of debug performance in c++ Improving the State of Debug Performance in C++ Personally I would just like to see less std:: and less meta-programming in the code I work in, but since I work in reality it is kind of hard to avoid so I think work being done on making these kind of things cheaper is very welcome!\nIt might also be worth noting that I quickly tested out some \u0026ldquo;auto-vectorization\u0026rdquo; pragmas and that kind of stuff as well and at a first glance it didn\u0026rsquo;t change the generated code one bit. I might have done something wrong or just missed something, I don\u0026rsquo;t think so but I have been proven wrong before :D\nAlso, during me writing this post gcc was updated in my ubuntu dist and I saw that some of the perf-issues noticed in here had been fixed. We\u0026rsquo;ll see if it is noticeable enough to warrant me writing more on the topic!\nSo were do we go from here? There are a lot of things that might be worth looking into in followup posts, things such as:\nWhat do MSVC do? Is it worth looking into some inline assembly in some parts of the code? optimizing (and making sure they work as expected) the other functions in memcpy_util.h. Last words. Was this interesting? Where did I mess up? Want to see more like this? Hit me up on mastodoon and/or \u0026ldquo;the bird site\u0026rdquo; and tell me! (As long as you are fairly nice!) If something interesting pops out I might do a followup :)\nApendix Appendix with asm-listings and tables!\n"
            }
    
        ,
            {
                "id": 5,
                "href": "https://kihlander.net/post/a-zig-diary/",
                "title": "A Zig Diary",
                "section": "post",
                "date" : "2023.01.08",
                "body": "As it turned out I happened to help out with fixing a space to host a zig meetup here in Stockholm at the place where I work. However I haven\u0026rsquo;t written a single line of zig in my life\u0026hellip; I felt that it might be worth doing something about that :D\nIf you don\u0026rsquo;t know, zig is a new systems programming language!\nI have found zig quite intriguing for a while now but I haven\u0026rsquo;t had the time to look into it so this sounded like an as good excuse as any! I\u0026rsquo;ll try something new with this post and just write as I go and document my success/failure/reflections, this might make it a bit \u0026ldquo;rambly\u0026rdquo; :)\nWhat to build? First off, I needed to decide on what to try and build just to have a goal and not just do some aimless \u0026ldquo;twatting about\u0026rdquo;, something small but that still produce something. I decided on a simple image to ascii converter. There are a multitude of these around that will probably be better than this one but who cares!\nI decided on this as its small, does a bit of io, does a bit of command-line, a bit of \u0026ldquo;algorithm work\u0026rdquo; and probably some c-interop, and it could probably be optimized a bit if I want to.\nLets go!\nStep 1\u0026hellip; build something? I needed to get something building! The first instinct said \u0026ldquo;Let\u0026rsquo;s hit google and search for \u0026lsquo;zig how to get started\u0026rsquo;\u0026rdquo;.\n5 minutes later I had the compiler and toolchain installed, initialized my project and have something running\u0026hellip; impressive!\nA bit more info might be interesting to you readers! So my google search turned up https://ziglang.org/learn/getting-started/ as the first result, that in turns pointed me to the downloads page. As I\u0026rsquo;m on ubuntu I decided to do the \u0026lsquo;snap\u0026rsquo; install via:\nsnap install zig \u0026ndash;classic \u0026ndash;edge\nAfter this the getting started page told me about zig init-exe and zig build run and I was off to the races!\nStep 2\u0026hellip; syntax highlighting! Can\u0026rsquo;t write code without a decent editing environment, so let\u0026rsquo;s see what we can do about that?\nI know that the \u0026ldquo;getting started\u0026rdquo; page talk about this, but vscode as I use as an editor at home already points me to the marketplace, lets start there.\nInstalling \u0026ldquo;Zig\u0026rdquo; and \u0026ldquo;Zig Language Server (zis) for VSCode\u0026rdquo; and lets see what happens!\nAnother 5 min and syntax is highlighting and some rudimentary auto-completion is in place!\nStep 3\u0026hellip; time to write something by myself. Reading the initial generated main.zig, clear and concise but short. Nice that there is a test embedded in the generated code as well. I have never been a fan of having tests and implementation in the same file but we\u0026rsquo;ll see how that turns out, especially since testing seems builtin to the language. I guess there is nothing that force me to have the test in the same file as implementation?\nTrying to uncomment the .deinit() in the test to see what happens.\nvar list = std.ArrayList(i32).init(std.testing.allocator); // defer list.deinit(); // try commenting this out and see if zig detects the memory leak! try list.append(42); try std.testing.expectEqual(@as(i32, 42), list.pop()); Good to get an early introduction to memory-handling this early, otherwise it might have come as a surprise. I\u0026rsquo;m usually a fan of manual memory-management but also working in c++ all day long at work might make this something to get used to. Also what are these try-statements, defer I get (and like!), but try is a bit unclear, might have been good with a bit of clarification directly in the generated file? Just a quick comment like:\n// adding a \u0026#39;try\u0026#39; here will do x/y/z. // Read more at \u0026#39;https://ziglearn.org/chapter-1/#errors\u0026#39; So, what is the first thing that I need? I\u0026rsquo;ll need to read a data from stdin and or via an -i-flag. First I thought that I should use some lib, but I\u0026rsquo;m learning zig now, just do the simplest thing you can yourself in this simple case!\nSo getting argc/argv? A few questions and reflections popped up.\nDebug-printing something and getting this callstack is kind of bad, sure it tells me what is wrong but it gives me an error in the standard-lib and not in my code where I introduced the issue. It also hides my line from the trace, so I can\u0026rsquo;t even see the line where I introduced the error!\nAt least it mentions -freference-trace, but as a user this might not be what I want :)\nwc-duck@WcLaptop:~/kod/zig_img_to_ascii$ zig build run /snap/zig/6044/lib/std/fmt.zig:86:9: error: expected tuple or struct argument, found [:0]const u8 @compileError(\u0026#34;expected tuple or struct argument, found \u0026#34; ++ @typeName(ArgsType)); ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ referenced by: print__anon_4373: /snap/zig/6044/lib/std/io/writer.zig:28:27 print__anon_3784: /snap/zig/6044/lib/std/debug.zig:93:21 remaining reference traces hidden; use \u0026#39;-freference-trace\u0026#39; to see all reference traces Issue introducing this was just me being used to \u0026ldquo;c\u0026rdquo; for a long time and writing:\nstd.debug.print(\u0026#34;{s}\\n\u0026#34;, arg); and not:\nstd.debug.print(\u0026#34;{s}\\n\u0026#34;, .{arg}); This line is the most important thing to me as a user, hence it should be mentioned in the actual error!\nNext up, can I run zig build run with command line args? Couldn\u0026rsquo;t find anything in the huge zig --help. Also tried to do something as zig run help to just get help on run but no. After a bit of guessing I found that zig build run -- --arg --go --here works and pass all args after -- to your app. Not really unexpected as it is kind of a known idiom , but also not documented\u0026hellip; as far as I can tell?\nstd.debug.print() requires args and can not just print a string\u0026hellip; unexpected. Forcing a .{} is not really ergonomic :(\nstd.debug.print(\u0026#34;I just want some output\\n\u0026#34;, .{}); // :( Comparing strings threw me, as many others it seems, off as there are no standard way doing it\u0026hellip; having to define your own streq() seems a bit excessive as it is not an unheard of operation!\nfn streq(comptime s1 : []const u8, s2 : []const u8 ) bool { return std.mem.eql(u8, s1, s2); } Sure, that is just a wrapper around std.mem.eql(), comparing strings is far from trivial if you are to do it \u0026ldquo;correctly\u0026rdquo; etc\u0026hellip;\nAt the time of writing this post I found that there is the std.ascii module, but that do not have a simple .eql(), only .eqlIgnoreCase(). I guess that comes from \u0026ldquo;there should be one, and only one way, of doing one thing\u0026rdquo; and std.mem.eql() already exists but I would argue that that is taking it a bit to far. From my point of view an std.ascii.eql(s1, s2) would provide more clarity at the callsite + making it easier to discover other \u0026ldquo;ascii\u0026rdquo;-functions etc.\nBut after some \u0026lsquo;faffing about\u0026rsquo;, commandline is parsing, probably really badly :)\nconst CmdLineArgs = struct { input: ?[]const u8 = null, }; fn parse_args(alloc : std.mem.Allocator) CmdLineArgs { var args = try std.process.argsWithAllocator(alloc); // skip my own exe name _ = args.skip(); var out = CmdLineArgs{}; while(args.next()) |arg| { if(streq(\u0026#34;--input\u0026#34;, arg) or streq(\u0026#34;-i\u0026#34;, arg)) { out.input = args.next(); } else if(streq(\u0026#34;--help\u0026#34;, arg) or streq(\u0026#34;-h\u0026#34;, arg)) { std.debug.print(\u0026#34;HEEELP\\n\u0026#34;, .{}); } else{ std.debug.print(\u0026#34;unknown arg {s}\\n\u0026#34;, .{arg}); } } return out; } Step 4\u0026hellip; loading images and c-interop Time to load a source-image, and as this is not an exercise in writing an image-loader and zig prides itself with c-interop, lets drop stb_image.h in there and get that running!\nThis actually turned out a bit harder than expected, mostly due to how stb_image.h is implemented with having to define STB_IMAGE_IMPLEMENTATION. This forced me to add an stb_image.c that just include stb_image.h and set the define.\nThis lead me down into the zig-buildsystem and build.zig and add this\u0026hellip; pretty sleek!\nexe.addCSourceFile(\u0026#34;src/stb_image.c\u0026#34;, \u0026amp;[_][]const u8{\u0026#34;-std=c99\u0026#34;}); exe.addIncludePath(\u0026#34;src\u0026#34;); exe.linkLibC(); But how did I find out how to do this, it basically took googling \u0026ldquo;stb_image zig\u0026rdquo; and ending up with Andrew:s tetris-implementation on github. Maybe not the best and clearest documented thing. At the other hand you probably shouldn\u0026rsquo;t need to officially \u0026ldquo;document\u0026rdquo; how to use a specific lib from zig :D\nHowever a short blog-post about it, and only it, would be useful\u0026hellip; maybe for kihlander.net?\nLeft the project for a few days\u0026hellip;\nStep 5\u0026hellip; produce some output! Unfortunately my note-taking was, to put it mildly, \u0026ldquo;lacking\u0026rdquo; under the actual implementation of the to-ascii work. But it is mostly a few loops and I\u0026rsquo;m pretty sure that you can find better resources on how to convert an image to ascii that this blog :)\nIn short, convert image to grayscale, use average in a block of x*x pixels to map into list of character.\nBut I do have \u0026ldquo;The king of all cosmos\u0026rdquo; as ascii \u0026ldquo;art\u0026rdquo; at least :)\nTurning this:\nInto this:\n.......;:.........;x:,oo..:%;.;o,................ ..................o%;,%x,.:%o.o%:................. ..................;x:,oo,.:x;,;x:................. ..................:o,,::,,,o,,:o...........,,..... ..................;%:,xo,,:%;,o%,..........:,..... .................,;x;:%%;,o%o:ox:................. .................,oxoo%#o:%%%;%x:,................ ..........,.....,,oo%x%#%x##xxxo;,...........,,... .........,;,....,,xxoo%#@@@#xooxo,..........,;;... ........:oo;,...,:xxoo%#@@##xooxx,.........,;oo:.. ........,;o;,..,,;xooo%#@@##xooxx,,.........:;;,.. ,.,....,.:;,..,,,;xooo%#@@##xooxx,,,.........:,... ,,,,,,,,,,,,,,,,,oxxoo%#@@##xooxx:,,,,,,,,,,,,,,,, ;;;;:;;;;;::;;:;;xxooo%%###%xooox;::::::::::;::::: ;;oo;;oo;;;;;;;;;;oo;;;;ox;;;;;oo:;;;;;;;;;;o;;;;; ooooo;oo;;;;;;;;;:x#%xo;ox;;ox%%o:;;;;;;;;;oo;;ooo oo%xoooo;;;;;;;;:;%###%xoxox%%##x:;;;;;;;;;oo;;oxo ooxoooooxxxxxxxo:o####%%%%%%%###%o;xxooooxxoo;ooxo ooooooox#######x;xxxooooxxoooooxxx:x%%%%%%%xoo;ooo ooooooxxxxxxxxxo;;::,,:::;;::,,:;;:xxxxxxxxxxooooo ooooxxoooooooooo;:::,,,:;o;:,,,:,;:ooooooooooooooo xxxxxooooooooooooo;;::;;x%o;:::;;o;ooooooooooooxoo xxxxxxoooooooxo:x%%xooxxxxxxooox%%o;oooooooooooxoo %xxxxxxxxxxxxxo;x%%%%%%oxxxx%%%%%%x:oxxxxxxxxxxxoo #xxx%@@@@@@@@#oo%##%%%xoxxxox%%%%%%;x@@@@@@@@@%xox %xx%%%#@@@@@@%:o%##%%%xxxxxoxx%%##%o;#@@@@@@#%%xxx xxx%%%%%%%%%%o:x%%%%xxoxxxxooxx%%%xo:o%%%%%%%xxxxo xxxxxxx%xxxxx;:xxxxxxooxxoxxoxxxxxxo,oxxxxx%xxooox xxxxxxxxxxxxx;:ooooooooxo;oxooooooo;,;xoooxxxooooo xxx%%xoxxxxxx;,;;;;o;ooo;;;oo;;;;;;;,oxxxxxooox%xo ox%#%xoox%@@#;,;;;;;;;::::,::;;;;;;:.o####xooo%#%o oo%@#xoox%@@#:.:;;:;:,...,...,:;,;;:.;@@@#oooo%@%o ooxxxo;ooxxxx:.:;;:,,:;;,,,;;:,,:;;,.;xxxxo;;oxxxo ;;;oo;;oooooo,.,:;:.:o;:...:;o:.:;:, :oo;;;;;;;;;; ;;;;;;;;;;:;;. .::;:;;.,,,,,,;;:;::. ,;;:::;;;;;;; :::;;;;;;;;;;. .,::;,:;ooooo:,::::.. ,;;:;:;;;:::: :::;;;;ooooo; ..::,,,,,,,,:::.. .;o;;;;;;;;:: ;:::;oxxxxxx; ,;::,....,,:::. ;xoooooo:::: ;::::;;;;;;;: ,:;;::,,:::;;:. :::;;::;::,, :,,:::::,,,:, ,:;oo:. .;oo;:. ::,,,,,,::,, ,,,::,,,,,,,,, ,:;xx;, :oxo;:. ,,,,,,,,,,,,, ,,,,,,,,,,,,,,,. .,:;;;: :;;;:,. .,,,,,,,,,,,,,, ,,,,,,,,,,,,,,,,, .,,,,, ,,,,,. ,,,,,,,,,,,,,,,, ,,,:::;;oxxo;::::, ...... .......,::::;oxoo;:::,,, ooxx%%%#@@@%oxx%%x;..... .....,;x%%xxo#@@@#%%xxoo @@@@@@@@@####@@@@@x:,..,:::, .,;%@@@@######@@@@@@@ @@@@@@@@@@@@@@@@@#%o;,;x###x;,;o%#@@@@@@@@@######@ @@@@@@@@@@@@@@@@#%xo;x#@@@@@%o;ox%##@@@@@@@@@@@@@# %x%#@@@@@@@@@@@#%%xo;%@@@@@@@x;ox%%##@@@@@@@@@@%%x xoox%@@@@@@@@@##%xxox##%xxx%@#o;ox%######@@@@@%ooo Ugly ascii art yes\u0026hellip; but still! And the code is as crapy as ever!\nIt looks better in my terminal :D But I think it comes down to tweaking the \u0026ldquo;alphabet\u0026rdquo; used, deciding how many chars to use in width etc. Nothing really interesting for this post.\nConclusion Time for a few reflections and conclusions.\nFirst off all, zig do not feel ready for \u0026ldquo;primetime\u0026rdquo; for me but I do feel that it has a promising future. It has many good ideas that sit right with me and seems well suited to the kind of work that I usually do. But would I suggest our team to go \u0026ldquo;all out zig\u0026rdquo; at this time\u0026hellip; hell no!\nThe good Good defaults I like the defaults when it comes to how the language is designed and how the builds are setup with Debug/ReleaseSafe/ReleaseSmall/ReleaseFast.\nMemory allocation is explicit As someone that has been coding c and c++ in this way for a long time (or at least tried to advocate for this style) it feel right at home having this builtin to the language with the tools needed to make it fairly streamlined. Forcing the user to take responsibility for memory is a good thing. But that is also said as a \u0026ldquo;systems programmer\u0026rdquo;, who\u0026rsquo;s responsibility it is to think about these kind of things on a daily basis. Would it go over as well with one of my colleges working in gameplay where iteration time is of an essence when prototyping things and design can change, radically, from hour to hour? Probably not?\ncomptime feels like a powerful paradigm comptime feels powerful. Being able to not have to use a \u0026ldquo;special\u0026rdquo; language such as templates for your generic code, I like that! It feels clear and easy to read and understand. But how is it to debug? As it is \u0026ldquo;just zig\u0026rdquo;, could there be some kind of special build or maybe a way to step through and evaluate generic code in a debugger? This would be such a powerful feature, to be able to use your day-to-day debugger to debug your compile-time evaluated code!\nBuild System Easy to get going and builtin as a first class part of the language. Not being an afterthought.\nThis worked really well for me on my small test-project and I guess it would work really well for bigger projects as well. But how would it scale to really big code-bases, especially with lots of custom build steps (looking at you Apex-engine, looking at you!), that is to be seen?\nTrying to \u0026ldquo;work well with\u0026rdquo; c instead of replacing c! I really like that zig is trying to work with the current landscape that is already there instead of trying to replace it from the ground up. There is just so much great and well established libraries and tools out there already. Making it easy to interact with these instead of trying to replace them sounds like the right choice to me.\nThings like zlib, curl and sqlite will probably not be re-written any time soon!\nThe bad Compile times. The compile times are far from \u0026ldquo;good\u0026rdquo;, to put it politely :( And this just for a small commandline app that would probably compile in non-noticeable time in c. Throwing a big project on this would probably not be sustainable today.\ntime to build, clean (no ./zig-cache and ./zig-out) on my machine:\nreal 0m11,741s\nuser 0m8,777s\nsys 0m5,789s\nYAUSA!!!\ntime to build, small update to \u0026lsquo;main.zig\u0026rsquo; on my machine:\nreal 0m1,074s\nuser 0m0,953s\nsys 0m0,183s\n\u0026hellip; for such a small change, not really \u0026ldquo;stellar\u0026rdquo; :(\nI know that this is being worked upon and with the new self-hosted compiler it has become (according to \u0026ldquo;the interwebs\u0026rdquo;) a lot better but im not sure how far you can take it?\nWith all comptime etc it will be a problem to get it fast. The computations will have to be done regardless and the more calculations that has to be done compile-time the more expensive it will be! But I\u0026rsquo;ll be happy to be proven wrong and it\u0026rsquo;s definitively possible that it can get to a decent state!\n@compileError, I don\u0026rsquo;t want errors down in the stdlib! Getting errors deep down in the stdlib when I made an error in my own code is not really good. See the error discussed above on sending invalid parameters to std.debug.print() and getting the error deep down in the stdlib.\nHow to address this, I don\u0026rsquo;t know. This is the same problem as static_assert() and template-errors in c++. However zig should be able to leverage that it has proper modules instead of glorified copy-paste that is c-includes :)\nI do see @compileError as a really good paradigm, but without this problem solved it feels like it is going to end up as the c++-style wall-of-text that you have to decipher in order to find the real error you made.\nHopefully this is at least seen as a problem by the zig-community and there should at least be things that can be done to make it less painful?\nPlease give me a way to iterate over an integer range!!! ARGH! The loops over a range of integers!\nvar block_h: u32 = 0; while (block_h \u0026lt; block_cnt_h) : (block_h += 1) { var block_w: u32 = 0; while (block_w \u0026lt; block_cnt_w) : (block_w += 1) { } } If I had a swedish \u0026ldquo;krona\u0026rdquo; for each bug this introduced during my short time with zig I would probably be able to afford a really cheap cup of \u0026ldquo;snut-kaffe\u0026rdquo; (cop coffee in swedish). This was really cumbersome to use and far from nice. Really clunky!\nHowever at the meetup I got to ask Loris Cro about this and got the answer that this is going to be addressed in the future, see https://kristoff.it/blog/zig-self-hosted-now-what/ under \u0026ldquo;New for loop syntax\u0026rdquo;! That one seem quite nice and will hopefully remove this gripe of mine in the future!\nI guess this, again, comes down to \u0026ldquo;there should be one, and only one way, of doing one thing\u0026rdquo; and in theory I find that to be a valiant goal but when it gets in the way of ergonomics like this it is far from good. However as it seems as if it is being addressed, the main developers seem open to change if it would really benefit the language as in this case! And it is always easier to give a small feature than remove something that turned out to be a mistake, so it is probably a good thing that features is just not thrown in there if they are not proven to be needed!\nLocal functions Nope, not there. This tripped me up a bit. This is honestly nothing major but personally I have started using local functions more and more and I think they make my code simpler and cleaner over all. I know that there has been discussions in the zig-community about it but I haven\u0026rsquo;t tried to find what the conclusions has been.\nDocumentation is good\u0026hellip; the documentation that\u0026rsquo;s there that is! Yeah\u0026hellip; the documentation. It is good! When you can find it and it exists :( Usually you end up in some blog, some github issue or similar. But zig is a young language, it is not that unexpected that documentation is lacking at this point in time!\nThe ugly Still fighting the syntax I\u0026rsquo;m still fighting the syntax. My 20 years of writing c and c++ trips me up all the time. This however I guess is more on me than the actual syntax\u0026hellip; maybe? There are quite a lot of usage of single characters like _, !, ?, : that make the code terse, but is quite the hurdle when you are new to the language.\nBut thanks for not using the dreaded backtick (`) for anything!!! Please go google \u0026ldquo;backtick scandinavian keyboard\u0026rdquo; for some \u0026ldquo;opinions\u0026rdquo; from us up north!\nthe preferred, idiomatic, format :) Again this is just a choice that is neither good or bad, just a choice\u0026hellip; that I would not have made! I don\u0026rsquo;t like it, but as it is a matter of taste it is kind of silly to complain about as no one will ever prove the other right as there is no right/wrong ( except when it comes to space-vs-tab and snake_case_being_the_way_god_intended() ;) )!\nWhere to next? So where do this take me? I will still keep an eye on zig and its development. Will it become my primary language of choice? Not yet, but maybe someday. I really hope/think that zig has a future and we will see more of it.\nIf nothing else and zig just disappears some day (not likely!) it will at least have brought interesting ideas to the programming world and proved/disproven that they work and what they can bring to the table!\nI do think that I will keep fiddling with zig in the future. I have had some thought going like \u0026ldquo;everyone has to have written an NES-emulator in their life right?\u0026rdquo;, and maybe it would be worth doing that in zig just for the fun of it?\nI\u0026rsquo;m also intrigued about trying out zig as a buildsystem for my data serialization library (we all got one right?) \u0026ldquo;datalibrary\u0026rdquo; and see how that turns out!\nIn the end\u0026hellip; this was kind of fun, would do again! And here, finally is a link to the crappy source for the img-to-ascii converter!\nhttps://github.com/wc-duck/zig_img_to_ascii\nHelpful resources https://ziglang.org/ https://ziglearn.org/ official doc The blog of Loris Cro - this must have been the one of my most used resources! "
            }
    
        ,
            {
                "id": 6,
                "href": "https://kihlander.net/post/when-memcpy-change/",
                "title": "When memcpy() change!",
                "section": "post",
                "date" : "2022.10.23",
                "body": " To start of, yes, I know that this article touch undefined behavior and that all bets are off!\nI am currently working on a bigger post on swapping memory that is THIS close to being done\u0026hellip; any day now (he has been saying the last year!).\nHowever this topic popped up and I was wondering if it was worth making the other post longer or just make a small one about it. As the other post is already quite big I opted for a shorter one here.\nSo what am I on about you might ask, weird title and all? memcpy() don\u0026rsquo;t just change right, it is well defined what it should do! It should copy memory from buffer a to buffer b\u0026hellip; as long as they don\u0026rsquo;t overlap, then your in undefined behavior territory (spooky sounds go here!).\nIn c and c++ we basically have 2 primitives to copy memory, we have memcpy() and memmove() where memcpy() is \u0026ldquo;as efficient as possible\u0026rdquo; and memmove() also handle the case where source and destination overlap and copy the data as if a temporary buffer was used in between.\nSo far so good, i.e. you, as a user of the functions, is expected to know if you have overlapping buffers in your src and dst.\nWell as it turns out, you and me as developers are quite bad att knowing if your buffers DO overlap or not, as seen by the kerfuffle back in 2010 when GLIBC decided to replace its old memcpy() that just used memmove() with an optimized version that really required the buffers not to overlap. \u0026ldquo;Hilarity\u0026rdquo; ensued and everyone had a great day at work\u0026hellip; or maybe I misunderstood something? ;)\nIMHO it is the right decision on a system-level to implement memcpy() as memmove() for the above mentioned reason, that reason being you and me as developers are stupid :) Many systems however don\u0026rsquo;t do this so we still need to think about it.\nAnd now to the interesting part\u0026hellip; lets try this out on my linux-machine. Let us add a simple program like this:\nIn this post I will define \u0026ldquo;the right\u0026rdquo; behavior to be memmove() all the time\u0026hellip; if this is right can absolutely be debated and I would not be on the side that it is ALWAYS the right choice, but for the sake of this article we define that as \u0026ldquo;right\u0026rdquo;.\nTrying it out #include \u0026lt;string.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; int main(int, const char**) { int cpy[] {0, 1, 2, 3, 4, 5, 6}; int mov[] {0, 1, 2, 3, 4, 5, 6}; memcpy (\u0026amp;cpy[1], \u0026amp;cpy[0], 5 * sizeof(int)); // OHNO... overlap ahoy! memmove(\u0026amp;mov[1], \u0026amp;mov[0], 5 * sizeof(int)); printf(\u0026#34;cpy: \u0026#34;); for(int c : cpy) printf(\u0026#34;%d \u0026#34;, c); printf(\u0026#34;\\n\u0026#34;); printf(\u0026#34;mov: \u0026#34;); for(int c : mov) printf(\u0026#34;%d \u0026#34;, c); printf(\u0026#34;\\n\u0026#34;); return 0; } play along in compiler explorer\nCompile with gcc\u0026hellip;\nwc-duck@WcLaptop:~/kod$ gcc test.cpp -o t wc-duck@WcLaptop:~/kod$ ./t cpy: 0 0 1 2 3 4 6 mov: 0 0 1 2 3 4 6 \u0026hellip; and success!\n\u0026hellip; and clang? \u0026hellip;\nwc-duck@WcLaptop:~/kod$ clang test.cpp -o t wc-duck@WcLaptop:~/kod$ ./t cpy: 0 0 1 1 3 3 6 mov: 0 0 1 2 3 4 6 OH NO!\nIf we dig in to the code that clang generates we see that clang has replaced the call to memcpy() and memmove() with its own inline implementations that DO follow the rules and expect input to memcpy() to not overlap. A valid conversion as it seems really wasteful to copy a few bytes via a function-call. (Question however, is clang really allowed to do this in a non-optimized build?)\nWe can also try an optimized build.\nwc-duck@WcLaptop:~/kod$ gcc test.cpp -o t -O2 wc-duck@WcLaptop:~/kod$ ./t cpy: 0 0 1 2 3 3 6 mov: 0 0 1 2 3 4 6 wc-duck@WcLaptop:~/kod$ clang test.cpp -o t -O2 wc-duck@WcLaptop:~/kod$ ./t cpy: 0 0 1 2 3 4 6 mov: 0 0 1 2 3 4 6 Now we see that gcc generate an invalid(?) result but clang generate what would be expected(?). This time gcc decided to replace memcpy() with an inlined version that is a \u0026ldquo;pure\u0026rdquo; memcpy(). But how did clang get it \u0026ldquo;right\u0026rdquo;, lets dig in?\nIt seem like the diff on clang between -O0 and -O2 is that it uses vector-instructions to implement its inlined memcpy() and memmove() and unroll the loop here as there are no branch-instructions in the copies that should occur\u0026hellip; I\u0026rsquo;m not really sure what is going on here, but we can at least conclude that in this case clang generate different code that result in different semantics depending on optimization flags. I guess we are in undefined behavior land now ¯\\(ツ)/¯\nConclusions? So what have we learned? Don\u0026rsquo;t know really\u0026hellip; undefined behavior is undefined maybe?\nOn a more serious note, should we be able to expect our compilers to generate equivalent code depending optimization level? One might think so but I guess that would have a lot of repercussions in other areas.\nAnd in cases like this where the compiler could easily see that the buffers overlap, should they generate memmove()-semantics code here? I know that some static-analyzers warn about this, but for all old code, why not try to at least save them from them self (and emit a warning)? I would be surprised to find code where the undefined behavior of memcpy() would be what you actually wanted?\nRegardless, I found this little \u0026ldquo;thing\u0026rdquo; was worth a few words in blog-form just for the sake of it\u0026hellip; hope you found it at least worth your time reading it :)\n"
            }
    
        ,
            {
                "id": 7,
                "href": "https://kihlander.net/post/macros-and-lambdas/",
                "title": "Macros and Lambdas",
                "section": "post",
                "date" : "2022.10.16",
                "body": "Time for a short post on using lambdas to construct macros\u0026hellip; that was a sentence that will be able to trigger 2 camps in one go :D\nDefer First of, using lambdas to implement a defer() is really neat, however others has already written about that so that I don\u0026rsquo;t have to!\nA Defer Statement For C++11\nCall once So from my end I\u0026rsquo;ll start of with a quick one for constructing a macro that only does something once, lets call it IS_FIRST_CALL(). This can be used for things such as only logging something once or just asserting once. I\u0026rsquo;ll leave it to the reader to decide if this is a \u0026ldquo;good\u0026rdquo; thing but it is absolutely things I have seen \u0026ldquo;in the wild\u0026rdquo;.\n// ... it can be used to implement other macros ... #define PRINT_ONCE(s, i) \\ do { \\ if(IS_FIRST_CALL()) \\ printf(s \u0026#34; %d\\n\u0026#34;, i); \\ } while(false) int a_function(int i) { if(i \u0026gt; 43) PRINT_ONCE(\u0026#34;first time i was bigger than 43 it was %d\u0026#34;, i); // ... or by itself ... if(i \u0026lt; 43 \u0026amp;\u0026amp; IS_FIRST_CALL()) printf(\u0026#34;first time i was smaller than 43 it was %d\u0026#34;, i); } A implementation of this would be something like this:\n#define JOIN_2(x, y) x##y #define JOIN_1(x, y) JOIN_2(x, y) #define JOIN(x, y) JOIN_1(x, y) #define IS_FIRST_CALL() \\ [](){ \\ static bool JOIN(call_it, __LINE__) = true; \\ bool JOIN(call, __LINE__) = JOIN(call_it, __LINE__); \\ JOIN(call_it, __LINE__) = false; \\ return JOIN(call, __LINE__); \\ }() We use a lambda (i.e. introduce a local function) to enable us to declare and check a static variable in any scope and \u0026ldquo;join\u0026rdquo; in the line-number to make sure that we don\u0026rsquo;t get warnings for variable \u0026ldquo;shadowing\u0026rdquo;.\nNot much code, but increasing readability according to me!\nSilence unused variables Next one!\nIn the codebase\u0026rsquo;s where I usually work we treat unused variables as errors (for better or for worse!), imho this is usually valuable as it help with getting rid of dead code. However it do introduce issues with things such as logging and asserts where variables and functions only become unused in specific configs.\nConsider something like this, where we have a PRINT() function that can be disabled with a define.\n#if !defined(SHOULD_PRINT) #define PRINT(fmt, ...) // NOTHING! #else #define PRINT(fmt, ...) printf(fmt, __VA_ARGS__) #endif int main(int, char**) { int var = 0; PRINT(\u0026#34;an int %d\u0026#34;, var); return 0; } Compiling and running this works just fine \u0026hellip;\nwc-duck@WcLaptop:~/kod$ clang++ t.cpp -Wall wc-duck@WcLaptop:~/kod$ ./a.out an int 1337 \u0026hellip; until someone disables the printing!\nwc-duck@WcLaptop:~/kod$ clang++ t.cpp -Wall -DNO_PRINT t.cpp:63:9: warning: unused variable \u0026#39;var\u0026#39; [-Wunused-variable] int var = 1337; ^ 1 warning generated. Time to introduce SILENCE_UNUSED(...)\n#define SILENCE_UNUSED(...) \\ do { \\ if(false) \\ [](...){} (__VA_ARGS__); \\ } while(false) This can then be used to implement PRINT() or by itself!\n#if !defined(SHOULD_PRINT) #define PRINT(fmt, ...) SILENCE_UNUSED(__VA_ARGS__) #else #define PRINT(fmt, ...) printf(fmt, __VA_ARGS__) #endif There are however quite a bit to unpack here, why is all the different parts needed.\nFirst, we define a lambda taking variadic arguments, thus being able to use the argument without giving them a name\u0026hellip; it turns out compilers have a hard time reporting warnings for variables they can\u0026rsquo;t be sure that they exist ;) This could of course also be possible with a [[maybe_unused]] void silence_me(...) {} but I prefer a lambda here to not \u0026ldquo;pollute\u0026rdquo; the global namespace with an implementation detail.\nSecondly we need to put the call to the lambda within an if(false) to make sure that the actual arguments isn\u0026rsquo;t evaluated. We wouldn\u0026rsquo;t want a SILENCE_UNUSED(expensive_call()) to actually call expensive_call() do we!\nLastly we wrap all of it in the mandatory do {} while(false) to make the macro into a \u0026ldquo;proper\u0026rdquo; statement that is useable within if/else etc.\nConclusion So\u0026hellip; there we have it, some simple tools to build your macros using lambdas! Personally I find them kind of neat and I think they server a purpose!\n"
            }
    
        ,
            {
                "id": 8,
                "href": "https://kihlander.net/post/when-selecting-a-theme-is-your-biggest-problem/",
                "title": "When selecting a theme is your biggest problem!",
                "section": "post",
                "date" : "2022.07.23",
                "body": "I have been using pellican to generate and maintain my blog sinces its inception and it has been mainly fine. However after getting a new machine and starting to fiddle with a new post I started noticing things such as installation being far from optimal and it no longer generating a functioning site after updating pellican.\nAfter an update all line-breaks in code-segments just got removed, did I configure something wrong, what to do to fix it? Don\u0026rsquo;t know and Im not really keen on finding out.\nThis looked like a perfect time to try out something new and since hugo seems to be the \u0026ldquo;hottest thing around\u0026rdquo; I went for that\u0026hellip;\nAnd such a treat it was! As the title states, the hardest thing of this migration was to actually select a theme I liked (ended up with harbor by Masaya Watanabe as you might see :))! Everything else was just a smooth. Things such as:\nFrom having a single binary to \u0026ldquo;install\u0026rdquo; Really simple setup Great get started-doc Smooth builtin server with auto-update on content updates. Being markdown-based, making it trivial to switch from pellican. Simply a treat to work with!\nDon\u0026rsquo;t think I have anything else to add and I hope to be able to roll out some more \u0026ldquo;meaty\u0026rdquo; posts in the future. I have one in the making that has been sitting, waiting to be completed, since christmas that I hope to publish soon.\n"
            }
    
        ,
            {
                "id": 9,
                "href": "https://kihlander.net/about/",
                "title": "About",
                "section": "",
                "date" : "2022.07.19",
                "body": "Who I am I\u0026rsquo;m Fredrik Kihlander, engine programmer at Avalanche Studios by day, father and avid amateur cook at night.\nThe goal of this blog is to have somewhere to write down code-related things that might be of interest to others. Mainly C++ ( with a really light emphasis on the ++:es ), maybe some python and general game-dev stuff.\nDon\u0026rsquo;t expect regular updates!\n"
            }
    
        ,
            {
                "id": 10,
                "href": "https://kihlander.net/post/protothreads-with-a-twist/",
                "title": "\"ProtoThreads\" with a twist.",
                "section": "post",
                "date" : "2018.11.20",
                "body": "For a long time I\u0026rsquo;v been interested in running game-specific/entity-specific code in coroutines. Something like the following.\nvoid some_game_object_behavior( entity ent, ... ) { pnt3 points[] = { {1,1,1}, {2,2,2}, {3,3,3}, {4,4,4} }; int pos = 0; while(entity_alive(ent)) { // move the entity to a position and yield coroutine while movement is ongoing. move_to(ent, points[pos % ARRAY_LENGTH(points)]); pos++; for(int i = 0; i \u0026lt; 4; ++i) { shoot_bullet(ent); wait_sec(ent, 2); // do nothing for 2 seconds and yield the coroutine for that duration. } } } The above example is slightly simplified but I hope that it get the point across, that I want to be able to suspend code-execution at specific points and wait for certain actions to complete. Writing one-off game-code in this fashion might be a good way to work, especially when adding wait_for_any() and wait_for_all() etc.\nSo when I finally decided to take a stab at trying that out I started out by looking at how to implement the actual coroutines. There are a couple of libraries out there that I looked at for coroutines in c/c++ such as:\nlibaco libdill libmill Both libdill and libmill feel too \u0026lsquo;opinionated\u0026rsquo; on how they want you to structure your code ( not that weird since the both sets out to re-implement go:s goroutines in c ) and also feels \u0026lsquo;heavier\u0026rsquo; than what I need. libaco however sparked my interest, it looked quite lean and not too opinionated, i.e. it looks really nice! But there is a big BUT, no windows support yet =/ I do most of my development on Linux but throwing Windows support out of the \u0026ldquo;window\u0026rdquo; (badumtish) is not something I want to do. According to the issue-tracker it is in \u0026ldquo;the pipe\u0026rdquo; but it is not supported as of writing this.\nThis lead me to fire of a question on twitter about alternatives and where @RandyPGaul pointed out something that I had looked at before but totally forgot about, coroutines/protothreads based on \u0026ldquo;duff\u0026rsquo;s device\u0026rdquo;.\nSince this technique is already well documented on the interwebs I\u0026rsquo;ll just link to the original article here and wait until you have read it ( if you haven\u0026rsquo;t already ).\ncoroutines in c\nRead it yet? Good!\nHere are a few other links to libs that implement these kind of coroutines.\nprotothreads zserge/pt cute_coroutine What I like about this solution is that it is \u0026ldquo;just code\u0026rdquo; so it should in theory work on any platform without platform-specific code. It should also work with emscripten ( that I shall get running any day now!!! ;) ).\nThere are however things that I am missing from these otherwise fine libraries. A major one is based in how I plan to use them. I plan to have all my \u0026ldquo;behaviors\u0026rdquo; run one coroutine and have some kind of simple \u0026ldquo;scheduler\u0026rdquo; run them. Something simple as having all \u0026ldquo;active\u0026rdquo; coroutines in a list, remove them from the list when waiting for something, and update them in a loop. However doing that with the above libs would require me keeping track of what functions are associated with each coroutine state.\nAlso local variables/state is not handled by the above libs and would need to be handled by passing in some kind of state-struct, that would need to be different for each \u0026ldquo;behavior\u0026rdquo; and also be tracked by the above mentioned system.\n2 of the 3 libs also fails to handle calling another coroutine-function from within a coroutine and by that having the sub-call control the state of the top-level coroutine. For example if a sub-call does a yield or wait the entire call-hierarchy should do the same (cute_coroutine.h solves this for a fixed depth of sub-calls).\nSo what do you do when you have an interesting itch to scratch? You scratch it of course :)\nSolving the issues! As any decent NIH-addict I decided to try myself and see what I could do and came to the conclusion that all the above issues can be solved by adding a small stack to each coroutine, i.e. almost do what the compiler does!\nIntroducing my boringly named coroutine lib/header, coro.\nAs mentioned above the only real difference between coro and the above mentioned libs are that each coroutine in coro MAY have a stack associated with it where the system itself can push data and reset when a coroutine completes.\nWarning for you C-all-the-way people, there be some usage of C++ in this piece of code! But I guess it wouldn\u0026rsquo;t be that hard to C:ify the lib if there is demand for it!\nThe library does nothing particularly fancy at its core, the simplest coroutine would be implemented and updated like this.\nvoid my_coro( coro* co, void*, void* ) { // all coroutines need a matching co_begin/co_end-pair co_begin(co); // ... do stuff ... // ... yield execution of coroutine, i.e. introduce a yield-point in the // function where to continue execution on the next update ... co_yield(co); co_end(co); } void run_it() { // ... create and initialize our coroutine ... coro co; co_init(\u0026amp;co, nullptr, 0, my_coro); // ... resume until completed ... while(!co_completed(\u0026amp;co)) co_resume(\u0026amp;co); } But now to the meat of this post, how will adding a stack solve the above mentioned issues?\nGeneral coroutine update Well, this isn\u0026rsquo;t solved by the stack, this is just solved by introducing a struct for the coroutines and storing a function-ptr in it :) Now we have that out of the way, carry on.\nLocal variables Lets get to the stack-part and start with local variables. When we have a memory-area to store data in the problem isn\u0026rsquo;t really to store the data, it is how to make it nice to use.\nAs all the above mentioned libs, the same goes for coro, you can\u0026rsquo;t just make a local variable and expect it to work\nvoid some_game_object_behavior( coro* co, void*, void* ) { int my_counter = 0; co_begin(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++my_counter); co_yield(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++my_counter); co_yield(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++my_counter); co_yield(co); co_end(co); } One might expect for this to print:\nwhoo 1\nwhoo 2\nwhoo 3\nBut it will print\nwhoo 1\nwhoo 1\nwhoo 1\nWhy? If you read the article coroutines in c ( you did read it right? ) then you see the problem. The coroutines build on calling the function over and over until it exits at the end. On each call it will initialize a local variable to 0, jump to the last position in the function, increment and print.\nThats not good now, is it? \u0026ldquo;Didn\u0026rsquo;t you mention solving this with the stack\u0026rdquo; you might think and that is correct. coro has a pair of functions (actually a macros) called co_locals_begin()/co_locals_end() that is used like this\nvoid some_game_object_behavior( coro* co, void*, void* ) { co_locals_begin(co); int my_counter = 0; // could be any amount of variables here! co_locals_end(co); co_begin(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++locals.my_counter); co_yield(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++locals.my_counter); co_yield(co); printf(\u0026#34;whoo %d\\n\u0026#34;, ++locals.my_counter); co_yield(co); co_end(co); } These macros will declare a local struct and instantiate a reference to one of these called locals, and guess where that reference is pointing, into the stack! This variable will only be allocated from the stack when entering the function for the first time, in the following calls it will just be fetched from the stack.\nWhat this means is that we will have a struct that will be the same between all calls to our coroutine, that is not exposed to the calling code and take the burden of keeping track of this away from the caller.\nIt might be interesting to have a look at how the macro work as well, if we just expand it and look at what is generated.\nvoid some_game_object_behavior( coro* co, void*, void* ) { struct _co_locals { int my_counter = 0; }; if(co-\u0026gt;call_locals == nullptr) { co-\u0026gt;call_locals = _co_stack_alloc( co, sizeof(_co_locals), alignof(_co_locals)); new (co-\u0026gt;call_locals) _co_locals; } _co_locals\u0026amp; CORO_LOCALS_NAME = *((_co_locals*)co-\u0026gt;call_locals); co_begin(co); // ... function ... co_end(co); } As you can see it just declare a local struct and put everything between co_locals_begin()/co_locals_end() into it. Then, if it is the first call, allocate data from the stack at the correct size/alignment. By placing the values in a struct we can put all of this in one declare call + we get size/alignment of the entire block for free from the compiler.\nAlso, since c++ now supports \u0026lsquo;inline\u0026rsquo; initialization ( I guess there is a fancier name for it ) of member-variables we can just write out or variables, set initial values and use placement new to initialize the values.\nNote to the C++:ers out there, currently no destructor is run on the locals but I guess that could be implemented in co_end() if needed.\nsub-calls With local variables out of the way, how about calling another coroutine function from the first one? Well, just to state the obvious calling an ordinary function is just doing the call if someone was wondering. However say that you want to call a function that can, by itself, yield execution?\nvoid some_game_object_sub_behaviors1( coro* co, void*, void* ) { // ... do stuff ... wait_for_timer(); // how this is implemented is up to the user ;) // ... do other stuff ... } void some_game_object_sub_behaviors2( coro* co, void*, void* ) { // ... do other cool stuff ... wait_for_timer(); // how this is implemented is up to the user ;) // ... DAMN THIS IS SOME COOL STUFF GOING ON HERE ... } void some_game_object_behavior( coro* co, void*, void* ) { co_begin(co); // ... function ... if(rand() % 1) { // some_game_object_sub_behaviors1()? } else { // some_game_object_sub_behaviors2()? } co_end(co); } Lucky for us we have the stack and co_call()! co_call() will allocate a coro-struct on the coroutine-stack and execute that just as any other coroutine. However it has some differences from co_init()+co_resume(). First of all, if it returns on the first call the caller will not yield, it will just continue. If it do yield it will be resumed by co_begin() of the caller until it completes and then the caller will continue at the yield-point introduced by co_call(). The resume of the sub-call could also have been done in the top-level co_resume() call but I decided to do it from the caller just to preserve the callstack for debugging. When the sub-call completes the stack will be reset to the point where co_call() allocated its coro-struct.\nThe above code will then be\nvoid some_game_object_behavior( coro* co, void*, void* ) { co_begin(co); // ... function ... if(rand() % 1) co_call(co, some_game_object_sub_behaviors1); else co_call(co, some_game_object_sub_behaviors2); co_end(co); } call-arguments So how about argument to coroutines? You guessed it, lets just pass them on the stack! Both co_init() and co_call() has versions that accepts a pointer to an argument + size/alignment. Example\nvoid some_game_object_move_on_path( coro* co, void*, int* path_index ) { move_me_on_path(*path_index); // maybe this will yield until movement is complete! } void some_game_object_behavior( coro* co, void*, void* ) { int path_to_take; // need to be declared before co_begin(), see below =/ co_begin(co); // ... function ... path_to_take = rand() % 5; co_call((co_func)some_game_object_move_on_path, \u0026amp;path_to_take, sizeof(int), alignof(int)); co_end(co); } The above will allocate space for the int and copy it onto the stack, and run some_game_object_move_on_path. The last argument to a co_func will be its arguments, or nullptr if not used. An alert reader might have noticed that the argument is copied onto the stack and that is true\u0026hellip; and its copied by memcpy so keep the arguments simple. I guess you could add lots of c++ magic to move types and yada yada but I haven\u0026rsquo;t needed that. IHMO keeping types memcpy-able usually keeps code simpler and easier to work with!\nNote: there is also a version of co_call() and co_init() that deduce sizeof() and alignof() from arg.\nAgain, no destructor will be run for the argument! Lastly note the cast to (co_func)! I guess that this is not everyones cup of tea but personally I\u0026rsquo;d rather take the cast there than in the function itself but I guess that is a matter of taste.\nRunning out of stack! So what happens when/if we run out of stack, for example allocating locals, args or doing a co_call()? coro will handle that gracefully and yield the coroutine at a point before the point of allocation. When this happen co_resume() will return as usual and the state can be checked by co_stack_overflowed() and that can then be handled by the user. The simplest might just be to ASSERT() but there is also co_stack_replace() if you feel fancy and want to grow the stack.\nAn example of how this might work\nvoid run_me() { uint8_t original_stack[128]; coro co; co_init(\u0026amp;co, original_stack, sizeof(original_stack), some_func); while(!co_completed(\u0026amp;co)) { co_resume(\u0026amp;co); if(co_stack_overflowed(\u0026amp;co)) { void* old = co_stack_replace(\u0026amp;co, malloc(co.stack_size * 2), co.stack_size * 2); if(old != original_stack) free(old); } } if(co.stack != original_stack) free(co.stack); } a note on \u0026lsquo;waiting\u0026rsquo; I mentioned above that I would like to be able to do things such as wait_for( timeout, move_to ). That is however something that I have mostly left out of coro. Why you ask? Well, I can\u0026rsquo;t really know how the user structures their code, how do they want to update the coroutines etc. If I would have supported something like that the lib would have become bigger and more \u0026lsquo;opinionated\u0026rsquo;, maybe it would have needed an update and a manager of some kind etc. That would have brought the lib from being a simple building-block to something more \u0026ldquo;framework:y\u0026rdquo; and that is not my intent. Maybe someday I\u0026rsquo;ll do something like that but then it will be built upon coro not built into it. There is however one small helper for building this kind of code, and that is co_wait(). co_wait() is basically a co_yield() that also sets a flag on the coroutine. This flag is also propagated through the currently running coroutine-callstack so that the user can do co_waiting(\u0026amp;co) at the top-level and see if it is waiting for something. This flag will be cleared on the next call to co_resume(). I think this little addition will be enough to build your own system on top of it if needed.\nConclusion First of all I need to say that I\u0026rsquo;m curious to see if it really works when I actually start to use it ;) I.e. my next task is to actually use this for something productive ( nope haven\u0026rsquo;t done that yet! ). IMHO it feels promising but we\u0026rsquo;ll see.\nSecondly let\u0026rsquo;s take a look at some pros/cons of this approach.\nPro: No platform specific code This is IMHO a big win for a smaller team ( in this case me ). No need to support low-level asm-code to switch stacks and save registers. No worries about \u0026ldquo;what happens if we start porting to a new platform, can we do the same thing there?\u0026rdquo;.\nPro: Not much code Also it is quite small ( at the time of writing 280 lines of code + 345 lines of comments ) and that is always a good thing for maintenance and \u0026ldquo;ease of use\u0026rdquo;.\nCon: Easy to mess up locals It is easy to mess up your local variables as it is second nature for every one of us to just declare a variable and expect it to keep its value :) My guess however is that you learn quickly and hopefully co_locals_begin()/co_locals_end() make it a bit easier.\nCon: Macro-heavy Personally I\u0026rsquo;m not that afraid of macros but I know some are. Also in this case they require you to follow quite a few rules and if you break them you end up with quite hard to understand errors. Again I think this is something that you learn but the more of you on your team the more people that have to learn and the fudge up a few times.\nAn macro-related error that can be quite hard to understand if you are new to the code is this\nvoid some_game_object_behavior( coro* co, void*, void* ) { co_begin(co); // ... function ... int path_to_take = rand() % 5; printf(\u0026#34;%d\\n\u0026#34;, path_to_take); co_yeald(co); co_end(co); } This code looks perfectly valid but generates this on my currently installed gcc.\ntest/test_coro.cpp: In function ‘void some_game_object_behavior(coro*, void*, void*)’: test/test_coro.cpp:43:16: error: jump to case label [-fpermissive] co_yield(co); ^ test/test_coro.cpp:40:9: note: crosses initialization of ‘int path_to_take’ int path_to_take = rand() % 5;\nAlso stepping through the co_***-macros while debugging is far from pleasant, hopefully that is my problem and not my users :)\nCon: no type-safety for arguments Currently there is no real type-safety between args passed to co_call()/co_init() and the actual function used as a coroutine callback. I would like to have that but I\u0026rsquo;m not really sure that it is doable? Any ideas for solutions would be appreciated ( easy on the meta-programming please ) !\nFinal note I think this turned out nicely and hope that its something that might be useful for some of you. On a bigger team with more resources, would I use this? Maybe? I think a full-scale stack-register switch might be a better solution but that has its own caveats ( TLS-variables for example ).\nAny thoughts or suggestions? I would love to hear about it! Please hit me up on twitter or post in the coro issue-tracker! And remember, there will be bugs, there always is!\nCheck it out https://github.com/wc-duck/coro!\n"
            }
    
        ,
            {
                "id": 11,
                "href": "https://kihlander.net/post/printf-based-tostr-on-the-stack/",
                "title": "printf-based TOSTR on the stack",
                "section": "post",
                "date" : "2018.05.15",
                "body": "As I might have written before I like printf-style string-formating. It\u0026rsquo;s imho the most convenient way to format strings and it can be really powerful if needed. However something that can be a bit tedious is output:ing \u0026ldquo;composite\u0026rdquo; values such as a vec3 or quaternion as there will be quite a bunch of repetition.\nprintf(\u0026#34;{ %.2f, %.2f, %.2f }\u0026#34;, vec.x, vec.x, vec.z); Doing this for multiple values really get verbose and its easy to make simple copy-paste-errors ( see above! ).\nI have read/looked at a whole bunch of string-formating libs and \u0026ldquo;tostr()\u0026rdquo; implementations, usually in c++, returning an std::string and/or overloading operator\u0026lt;\u0026lt; for writing to std::ostream etc and not really liking any of them ( forcing dynamic allocation for known sized outputs for example, yuck! ).\nBut complaining is easy, time to be constructive and get to some suggestions on how this can be done instead.\nBYTES2STR() I have found that most of these problems can be solved with a small trick, introducing a struct with the storage for the string. I will start introducing this with a simple macro to turn a size_t to a string in the format \u0026quot;234 bytes\u0026quot;, \u0026quot;2.9 kB\u0026quot; or \u0026quot;234 GB\u0026quot;.\nenum { BYTES_PER_KB = 1024, BYTES_PER_MB = 1024 * BYTES_PER_KB, BYTES_PER_GB = 1024 * BYTES_PER_MB }; struct _bytes_to_human { char str[64]; _bytes_to_human( size_t bytes ) { if ( bytes \u0026gt; BYTES_PER_GB ) snprintf( str, sizeof(str), \u0026#34;%.02f GB\u0026#34;, (double)bytes * ( 1.0f / (double)BYTES_PER_GB ) ); else if( bytes \u0026gt; BYTES_PER_MB ) snprintf( str, sizeof(str), \u0026#34;%.02f MB\u0026#34;, (double)bytes * ( 1.0f / (double)BYTES_PER_MB ) ); else if( bytes \u0026gt; BYTES_PER_KB ) snprintf( str, sizeof(str), \u0026#34;%.02f kB\u0026#34;, (double)bytes * ( 1.0f / (double)BYTES_PER_KB ) ); else snprintf( str, sizeof(str), \u0026#34;%zu bytes\u0026#34;, bytes ); }; }; #define BYTES2STR( bytes ) _bytes_to_human( bytes ).str This can now be used simply as follows\nprintf(\u0026#34;you have allocated %s\u0026#34;, BYTES2STR(allocated_bytes)); What we did here is to introduce a helper-struct containing the storage for the generated string and a macro to hide a bit of ugliness. Simple and easy to read and work with and I feel fairly confident when I say that the compiler will handle this will, NICE :)\nExtending to a more general TOSTR() But can this be extended to be a general and extensible solution? Would I write this article if I didn\u0026rsquo;t believe that it was? Of course it is and with a really small amount of code as well :)\nOur goal here is to be able to have one macro, TOSTR(obj), that take an object that has an implementation defined with TOSTR_DEFINE_TYPE_FUNCTION that defines how to convert the type to string and reserve space for that string on the stack.\nIf you just want the code or think it is easier to read the code directly here is a link to a github repo.\nhttps://github.com/wc-duck/tostring.\nOnto the code/implementation then!\nSo lets say that we have a vec3-struct that we want to add TOSTR() support to. That is as simple as using the macro TOSTR_DEFINE_TYPE_FUNCTION as follows.\nstruct vec3 { float x; float y; float z; }; // declare with type to implement for and the amount of bytes that // is needed in the output-string. TOSTR_DEFINE_TYPE_FUNCTION(vec3, 64) { // the end of the macro-expansion expects you to declare how // to write the output value. // // you will get passed an output-buffer as \u0026#39;out\u0026#39; ( with space // for the requested 64-chars ) and a reference to the value // to print as \u0026#39;value\u0026#39;. // // \u0026#39;out\u0026#39; has one member-function called \u0026#39;put\u0026#39; that has a // printf-like format. ( yes, that makes it out.put :) ) out.put(\u0026#34;{ %.2f, %.2f, %.2f }\u0026#34;, value.x, value.y, value.z); } The macro will declare a static constant for the declared type with the size like this, but for a few different cases ( vec3, const vec3, vec3\u0026amp; and const vec3\u0026amp; )\ntemplate\u0026lt;\u0026gt; struct _TOSTR_type_size\u0026lt;vec3\u0026gt; { enum { STRING_SIZE = 64 }; }; And it also declare a function with this signature ( using the code in {} after the macro as its body )\ninline void _TOSTR_type_impl( _TOSTR_builder\u0026amp; out, const vec3\u0026amp; value ); Finally we have the TOSTR(obj)-macro that will create a temporary buffer on the stack, fetching its size from _TOSTR_type_size\u0026lt;decltype(obj)\u0026gt;::STRING_SIZE and pass that to the _TOSTR_type_impl that will use overloading to select the correct implementation.\nWhat we can simply do now is just use TOSTR() as expected with vec3.\nprintf(\u0026#34;player: pos = %s, velocity = %s\u0026#34;, TOSTR(player_pos), TOSTR(player_vel)); // ... or ... printf(\u0026#34;player: pos = %s, velocity = %s\u0026#34;, TOSTR(player.pos()), TOSTR(player.vel())); that would output something like: \u0026quot;player: pos = { 11.0, 12.99, 13.37 }, velocity = { 0.14, 2.35, 3.71 }\u0026quot; ( depending on the actual position and velocity of the player hopefully ;) )\nbtw, they also \u0026ldquo;stack\u0026rdquo; quite well such as:\nstruct aabb { vec3 min; vec3 max; }; TOSTR_DEFINE_TYPE_FUNCTION(aabb, 128) { out.put(\u0026#34;{ %s, %s }\u0026#34;, TOSTR(value.min), TOSTR(value.max)); } Any negatives? The biggest draw-back in my opinion is the fact that the lifetime of the generated string is only during the current expression, i.e. you can\u0026rsquo;t \u0026ldquo;save\u0026rdquo; your string to use later. I.e. this is valid code but undefined behavior.\nconst char* vec_str = TOSTR(my_vec); printf(\u0026#34;%s\u0026#34;, vec_str); This since the macro defines a temporary variable to hold the value.\nAlso the fact that overflow might happen and you will get truncated output. I think this is a smaller problem as in many cases ( as the ones above and many more ) you actually know in advance the string-length that you will be generating.\nIf you find the overflow issue as a big issues it would be quite easy to add an \u0026ldquo;dynamic-alloc on overflow\u0026rdquo; to the system.\nI also know that some has problems with printf and friends all together as it is not \u0026ldquo;typesafe\u0026rdquo; etc. Imho that is mostly a solved problem by modern compilers that do type-checks in compile-time for you on these. But that is my opinion :)\nFinally there might be some objections to the template-usage in the implementation but I think it is at a reasonable level, but I guess that it can be solved in other ways if you think this is a problem.\nConclusion This might not be anything new but it is a small trick that I haven\u0026rsquo;t seen that much in the wild and that has made my own code \u0026ldquo;better\u0026rdquo; imho. It might not be to everybodys taste but hopefully someone has got a new tool to their toolbox!\nAs usual, don\u0026rsquo;t be afraid to reach out to me at twitter, github any other channel that you might know of!\n"
            }
    
        ,
            {
                "id": 12,
                "href": "https://kihlander.net/post/a-story-about-an-unexpected-abi-break/",
                "title": "A story about an unexpected ABI break",
                "section": "post",
                "date" : "2017.11.15",
                "body": "This is the story of an unexpected ABI break that I thought would be worth documenting.\nAt Avalanche we use a small class wrapping 32bit hashes called CHashString, it is basically just a wrapper around uint32_t and one should be able to treat it as a uint32_t in code except for operations that do not make sense on a hash-value.\nWhy would you want a class like this you might ask, well we use it for adding a const char* c_str()-function that can be used in logging and also we use it to add custom natvis-support in visual studio so that you can just hover a CHashString and have a lookup of the hash-value performed.\nHowever this is not about how we use it, but how things can break in unexpected ways.\nAs a bit more background it should be mentioned that a big part of Avalanches internal libraries are distributed pre-compiled to our game-projects with all the positives and negatives that brings with it. For example when deploying a middle-version fix we \u0026ldquo;promise\u0026rdquo; to our projects that we do not break the ABI of the library, i.e. you should be able to link with any 5.x.x if you only depend on 5.x.x.\nOur CHashString was basically defined something like this\nclass CHashString { uint32_t m_Hash; public: explicit CHashString(uint32_t hash) : m_Hash(hash) {} CHashString\u0026amp; CHashString(CHashString\u0026amp; other) { m_Hash = other.m_Hash; } ... more constructors ... ... more functions ... bool operator ==(const CHashString other) const { return m_Hash == other.m_Hash; } } As an earlier brain-fart/didn\u0026rsquo;t-think-about-that someone added the copy-constructor, something that made this class non-trivially-copyable, i.e. std::is_trivially_copyable would fail. This would lead to putting it in some containers would not make it as performant as it should have been ( and it couldn\u0026rsquo;t even live in some containers ).\nAs the fixy kind of guy one am I said to my self \u0026ldquo;I can fix this, how hard can it be?\u0026rdquo;. We decided that we should just remove that un-needed copy-constructor since a default copy-constructor would do the same thing. Said and done, be gone with you!\ncheck-in!\ndeploy!\ngo for coffee!\n\u0026hellip;\n\u0026hellip;\n\u0026hellip;\nCome back to crashing projects!\nSad panda!\nLuckily for me it is easy to lock down versions of distributed libs so we could quickly fix the issues on the projects by locking down to the previous version.\nAt this time we are scratching our heads quite a bit, our thinking being that even if one part of the code calls the old way of copy-constructing an object the end result should be the same in memory\u0026hellip; And to make things worse, most things seem to work.\nTime to bring out the debugger!\nI build a debug-build of one of our projects and after some time, thanks to some log-messages, I find a spot that behaves REALLY fishy!\nCHashString one_hash(0x12345678); CHashString another_hash(0x12345678); // ... later in the code ... if( one_hash == another_hash ) { do_stuff(); } do_stuff() is NEVER called!?! I.e. stuff is never done, and we all know that our job is mostly about getting stuff done ;)\nThe debugger tell me that the 2 values are the same! What is going on here? After checking the assembly and stepping the code quite a few times we can determine that when we removed the copy-constructor MSVC decided that it should pass CHashString in register instead of by pointer to stack. So what our operator== that take CHashString by-value ends up doing is comparing one of the hashes to half the stack-address of the other variable :)\nThis since this code is defined in one of our pre-compiled libraries and the implementation of operator== ends up in our main executable that is built from latest the lib and the exe disagrees on how to pass values to the function!\nAs expected this works in a release build where the code is inlined, but in that case we had other functions where how CHashString was passed was an issue.\nWhat can we learn from this? ABI-issues can show its ugly face when you least expect it and compilers do the darnedest things!\nWell, that was part of my day, how was yours? Feel free to hit me up on twitter or something if you want to give me a comment about this!\n"
            }
    
        ,
            {
                "id": 13,
                "href": "https://kihlander.net/post/builtin-resources-and-the-resource-system/",
                "title": "Builtin resources and the resource-system",
                "section": "post",
                "date" : "2017.08.07",
                "body": "I have written, in passing, about the resource-system and VFS ( Virtual File System ) I use in my own game-engine. This time it will however not be \u0026ldquo;in passing\u0026rdquo; but will dig down a bit deeper into one \u0026ldquo;feature\u0026rdquo; in it that I find kind of neat. I\u0026rsquo;m sure it has been done before but I have not seen it myself somewhere else. I\u0026rsquo;ll be writing about how I handle builtin resources in the engine.\nWhen talking about a \u0026ldquo;builtin resource\u0026rdquo; I refer to resources generated by code and not read from files on disk such as a red.png or a cube.mesh. Resources read from disk I\u0026rsquo;ll refer to as \u0026ldquo;assets\u0026rdquo;. Builtin resources might be useful while prototyping in cases like \u0026ldquo;Oh, I\u0026rsquo;m creating an new enemy-type but I need some place-holder mesh and material while testing and I just do not want to model a cube and paint a red texture\u0026rdquo;\nHow is the resource-system structured? I think that we will have to start with describing how the resource-system works, just to get some fundamentals down. In the bottom we have the VFS, or Virtual File System. That is just a system to hide access to \u0026ldquo;some kind of file-storage\u0026rdquo; ( file system, HTTP, archive-file etc ) where all files are referenced as an absolute path such as /assets/texture/apa.tex2d.\nThe VFS is in turn used by the resource-system that is a system with 2 purposes, creating/maintaining resource-containers and \u0026ldquo;turning VFS-paths to resources that can be used in engine\u0026rdquo;. I.e. load file via VFS and pass the data on to a callback, maintain a \u0026ldquo;handle\u0026rdquo; returned by said callback. A resource-container is just a collection of resources used to group resources such as base_resources and level_1_data etc.\nWell, as you can see it\u0026rsquo;s fairly traditional :)\nHow about them builtin resources then! So how do the engine handle the builtins then? Well, the paths to the VFS is always absolute and start at the root, i.e. start with a \u0026lsquo;/\u0026rsquo;. That opens up for \u0026ldquo;tagging\u0026rdquo; paths as special. So if the path is not starting with a \u0026lsquo;/\u0026rsquo; it is not a valid VFS path and we could use that in the resource-system itself. As I currently have it implemented the resource-system supports to path-formats:\n/absolute/VFS/path - a file path in the VFS. :res_type:type_specific_desc - a builtin for a specific type.\nSo if the path starts with, for example, :tex2: or :mesh: then the resource system just pass all that is right of the :res_type: to a specific create_builtin_callback for the resource-type registered for that type. Then it is up to the loading-code of the type ( tex/mesh/material etc ) to do what it pleases with the rest of the string. The callback for the specific type then just goes ahead an creates the desired resource and returns a handle to it in the exact same way as when it creates the resource from and asset or report an error back to the resource-system on error. I have ended up with just having all these paths \u0026lsquo;_\u0026rsquo;-separated and starting with a \u0026ldquo;base type\u0026rdquo; and followed by parameters, such as :mesh:cube_dim_(1,2,1)_pos_(0,1,0) to create a cube with dimensions 1,2,1 and center at 0,1,0 or :tex2d:solid_col_FF0000FF to create a solid red 2d-texture.\nPros and cons with the system pros Works with all systems referencing a resource type. I.e. you only have to implement \u0026ldquo;builtins\u0026rdquo; once for each type and whenever another system reference a resource it works with both assets and builtins automagically. Both \u0026ldquo;particles\u0026rdquo; and \u0026ldquo;renderables\u0026rdquo; can access builtin meshes via the same code for example. So when referencing a resource by VFS-path it will automagically work with builtins and by that works transparently from content.\nAutomatically hooks in to all parts of the resources system. You will get all your builtin resources to use the same allocators and the ordinary assets, instance sharing will work ( 2 \u0026ldquo;things\u0026rdquo; reference :tex2d:solid_col_FF0000FF and they will share texture-instance, debug-views showing loaded resources will show builtins, getting memory-statistics per container/resource-type works out of the box. In short, if it works with assets from disk it works with builtins.\ncons Resources need to be identified by strings. I would consider this the biggest issue and may make it unfeasible in a AAA-context where most of the times resources are identified by other types of ID:s. For most hobbyists, small-sized and mid-sized projects I don\u0026rsquo;t see that as a problem however as asset-counts and perf usually is \u0026ldquo;good enough (tm)\u0026rdquo; anyway.\nString-parsing and an undocumented \u0026ldquo;language\u0026rdquo; that is different per resource-type. This might also be a problem on a bigger team as the parsing need to handle errors well, warn clearly to the user what is wrong etc. Again on a smaller team where team-members work close together and problems ( such as error handling and error-output ) can be fixed quickly I think it should work out well. However take a note that it\u0026rsquo;s just a guess since my own projects is on a team of size 1 where all members think exactly like me ;)\nSafety? This might be an issue but nothing I have spent that much time on. As these codepaths might not be that well tested and hardened it might be a simple way \u0026ldquo;in\u0026rdquo; for a malicious person if you are concerned by that. In that case you might want to just have this enabled during development.\nConclusion I\u0026rsquo;ll just conclude by saying that this is a system/feature that has worked out really well for me on my own stuff and has proven itself to be really useful. As mentioned above it is really helpful to be able to reference \u0026ldquo;a red texture\u0026rdquo; or \u0026ldquo;a cube\u0026rdquo; directly via assets.\nWhat do you think? Has this been done before? Any other thoughts? Hit me up on twitter and tell me ( if you do it in a civil way of course! ;) )\n"
            }
    
        ,
            {
                "id": 14,
                "href": "https://kihlander.net/post/utf8_lookup-writeup/",
                "title": "utf8_lookup, a write up.",
                "section": "post",
                "date" : "2017.06.26",
                "body": "I saw this blog post a while ago A Programmer’s Introduction to Unicode, a really great write up that is a worth a read for anyone interested in the subject! So go ahead and read that now even as it might not be super important for what I am about to write about here :)\nReading this reminded me of an old project of mine that I think is a bit novel and deserves, at least, a write up. I am talking about utf8_lookup, a small lib to translate utf8 chars into offsets into a table.\nIt sprung out of the need to convert utf8-strings into bitmap-font glyhps for rendering. I.e. build some kind of data-structure out of a list of supplied codepoints and then use that to translate strings into a lists of indices of valid glyphs or 0 if the glyph wasn\u0026rsquo;t present in the original codepoint-list, i.e. rendering a bitmap font! That is what I have been using it for but I guess there might be other uses for a sparse lookup structure like this as well.\nIn short, this is what I have used it for (simplified):\nvoid render_text(const uint8_t* text) { utf8_lookup_result res[256]; size_t res_size = ARRAY_LENGTH(res); const uint8_t* str_iter = text; while( *str_iter ) { str_iter = utf8_lookup_perform_scalar( table, str_iter, res, \u0026amp;res_size ); for( size_t g = 0; g \u0026lt; res_size; ++g ) render_glyph( some_glyph_data[res[g].offset] ); // some_glyph_data might contain things such as uv:s etc. } } The properties of the lib are:\nlow memory usage one memory chunk for the lookup structure with all (one) allocations done by the user. fairly quick ( we will get in to this later under \u0026ldquo;performance\u0026rdquo; ) all non-found codepoints should map to offset 0 so that one can place a default-glyph there. However I hadn\u0026rsquo;t done any major profiling of the lib and I hadn\u0026rsquo;t compared it to some other approaches to doing the same thing. Time to change that!\nHow does it work? So now lets get to the meat of the post, how does it work?\nThe lib basically builds compact search tree based on the ideas of a Hash Array Mapped Trie, HAMT for short, where each level of the tree is based on each byte of an utf8-char.\nAs stated earlier the entire lookup structure is stored as one buffer, a buffer with the following layout.\nitem_cnt[uint64_t], availability-bits[uint64_t[item_cnt]], offsets[uint16_t[item_cnt]]\nitem_cnt the amount of items in the following lists, kept as uint64_t for alignment reasons. availability-bits collection of bit-fields of what codepoints that are available in the lookup per \u0026ldquo;octet-byte\u0026rdquo;. offsets depending on the octet of the current char to translate and step in the translation-process the next offset in availability-bits or start of result-offset. By octet I refer to how many bytes the current utf8-char is consisting of.\nThe first section of our availability-bits is the \u0026ldquo;root\u0026rdquo; of our lookup-table where a small table is used to find where to start the lookup. As octet one ( i.e. ASCII ) can have 128 possible values in utf8 ( first bit is 0 ) we need two slots in the availability bits where all other octets first byte fit in one 64-bit slots.\nSo what is stored is something like this: [ octet1 lower start ] [ octet1 higher start ] [ octet2 start ] [ octet3 start ] [ octet4 start ] \u0026hellip; [ octet2 bit1 ] [ octet2 bit2 ] \u0026hellip; [ octet3 bit1 ] \u0026hellip; [ octet3 bit1 bit2 ]\nWhat we are actually storing is info about groups of codepoints in subsequent bytes. I.e. we split the range of codepoints in chunks of 64 and if any char in that group is available in the table that bit is set. This is true except in the \u0026ldquo;leaf\u0026rdquo; where it represent if the actual codepoint exists. In short we have built a tree-structure with all levels having between 0 and 64 branches.\nAlso, if an entire 64-bits chunk is 0, we do not store it at all so there will be no 64-bit chunk that is 0, if there is I have a bug :)\nNow to find the actual offset that are to be our result, we store offsets to where the next 64-bit chunk for the next byte in our codepoint is stored. By then storing all subservient levels in the tree after each other as such:\ni will do this for 8 bits just to fit on the page :)\navail_bits[some_index] = [00010110] offset[some_index] = 16\navail_bits[16-18] = is the next level in the tree offsets[16-18] = offsets to the next level in the tree or final result-offsets.\nAs we only store one offset per level we need a way to find what item we need, but we do not want empty elements for the 0-bit. What we can do then is use base_offset + bits_set_before_checked_bit(), an operation that can be performed really fast by modern hardware via the popcnt-instruction and fairly quick with some smart code if popcnt is not available.\nSo the inner loop of the algorithm will look as follows.\n// table telling where to start a lookup-traversal depending on how many bytes the current utf8-char is. // first item in the avail_bits is always 0, this is used as \u0026#34;not found\u0026#34;. If sometime in the lookup-loop // a char is determined that it do not exist, i.e. a bit in the avail_bits-array is not set, the current // lookup index will be set to 0 and reference this empty bitset for the rest of the lookup. // // This was done under the assumption that you mostly do lookups that \u0026#34;hit\u0026#34; the table, i.e. you will need // to do all loop-iterations so instead of branching, just make the code always loop all iterations. // // if this is a gain is something to actually be tested. static const uint64_t START_OFFSET[4] = { 1, 3, 4, 5 }; // pos is the current position in the utf8-string to perform a lookup for. uint8_t first_byte = *pos; int octet = UTF8_TRAILING_BYTES_TABLE[ first_byte ]; static const uint64_t GROUP_MASK[4] = { 127, 63, 63, 63 }; static const uint64_t GID_MASK[4] = { 63, 31, 15, 7 }; uint64_t curr_offset = START_OFFSET[octet]; uint64_t group_mask = GROUP_MASK[octet]; uint64_t gid_mask = GID_MASK[octet]; for( int i = 0; i \u0026lt;= octet; ++i ) { // make sure that we get a value between 0-63 to decide what bit the current byte. // it is only octet 1 that will have more than 6 significant bits. uint64_t group = (uint64_t)(*pos \u0026amp; group_mask) \u0026gt;\u0026gt; (uint64_t)6; // mask of the bits that is valid in this mask, only the first byte will have a // different amount of set bits. Thereof the table above. uint64_t gid = (uint64_t)(*pos \u0026amp; gid_mask); uint64_t check_bit = (uint64_t)1 \u0026lt;\u0026lt; gid; // gid mask will always be 0b111111 i.e. the lowest 6 bit set on all loops except // the first one. This is due to how utf8 is structured, see table at the top of // the file. gid_mask = 63; ++pos; // index in avail_bits and corresponding offsets that we are currently working in. uint64_t index = group + curr_offset; // how many bits are set \u0026#34;before\u0026#34; the current element in this group? this is used // to calculate the next item in the lookup. uint64_t items_before = utf8_popcnt_impl( avail_bits[index] \u0026amp; ( check_bit - (uint64_t)1 ), has_popcnt ); // select the next offset in the avail_bits-array to check or if this is the last iteration this // will be the actual result. // note: if the lookup is a miss, i.e. bit is not set, point curr_offset to 0 that is a bitfield // that is always 0 and offsets[0] == 0 to just keep on \u0026#34;missing\u0026#34; curr_offset = ( avail_bits[index] \u0026amp; check_bit ) \u0026gt; (uint64_t)0 ? offsets[index] + items_before : 0x0; } // curr_offset is now either 0 for not found or offset in glyphs-table res_out-\u0026gt;offset = (unsigned int)curr_offset; Performance To test out the performance of the solution I have written a small benchmark app that is testing 4 different approaches to doing this and measuring some different stats\nmemory usage total amount of allocations speed GB/sec ms/10000 codepoints These benchmarks runs over quite a few texts in various languages. Downloaded from The Project Gutenberg. I have tried to get a good spread over different kind of texts using different combinations of code-pages etc.\nThe benchmarks will perform a complete \u0026ldquo;translation\u0026rdquo; of each text 100 times in a row.\nThe tested approaches for doing this are as follows\nuse utf8_lookup with a native popcnt instruction use utf8_lookup without a native popcnt instruction stuff all codepoint/offset pairs into an std::map stuff all codepoint/offset pairs into an std::unordered_map bitarray with native popcnt bitarray without native popcnt The bitarray-approach is just having a big array of uint64_t, set the bit if the codepoint exists, store an offset per uint64_t, and get the result-offset as:\nres = offsets[codepoint / 64] + bits_set_before(lookup[codepoint / 64])\nbasically utf8_lookup without compression.\nFinally its time for some numbers and charts!\nTexts used and results:\nfile codepoint count bpcp utf8_lookup bpcp bitarray bpcp std::map bpcp std::unordered_map ancient_greek.txt 222 0.891892 5.810811 40.0 30.342342 stb_image.h 95 0.505263 0.019531 40.0 23.242105 chinese1.txt 3529 0.971380 2.893171 40.0 24.451118 chinese2.txt 3540 0.959887 2.884181 40.0 24.424858 chinese3.txt 4226 0.818268 2.415996 40.0 30.209181 danish.txt 111 1.333333 11.351352 40.0 29.549549 germain.txt 133 1.413534 10.526316 40.0 27.308271 esperanto.txt 96 1.229167 13.437500 40.0 23.166666 japanese.txt 2176 1.506434 4.191961 40.0 28.232977 japanese2.txt 2438 1.506434 4.696691 40.0 29.705883 russian.txt 145 0.882759 8.896552 40.0 26.372414 big.txt 6069 0.607678 1.683968 40.0 25.894217 bpcp = bytes per codepoint\nAll tests has been run on my private machine, an Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz with 16GB DDR3 RAM running Ubuntu 14.04.5 LTS.\nAll builds has been done with g++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4.\nOptimized builds has been compiled with\ng++ -Wconversion -Wextra -Wall -Werror -Wstrict-aliasing=2 -O2 -std=gnu++0x -Wconversion -Wextra -Wall -Werror -Wstrict-aliasing=2 -O2\nAnd debug builds with\ng++ -Wconversion -Wextra -Wall -Werror -Wstrict-aliasing=2 -std=gnu++0x -Wconversion -Wextra -Wall -Werror -Wstrict-aliasing=2\nFirst lets get this out of the room, the std::map/unordered_map versions are really bad compared to the others in all measurements. That was expected but I added them to the tests as it is something that is the \u0026ldquo;first thing that comes to mind\u0026rdquo; and something I wouldn\u0026rsquo;t be surprised to see in a codebase.\nAs we can see the memory used and performance ( in GB text translated per second ) by the std::-implementations are just huge compared to the other to solutions, so lets remove them to get some more interesting charts :)\nThe charts clearly show that utf8_lookup outperforms the bitarray in all cases except pure ASCII ( stb_image.h ) when it comes to memory-usage and loses when it comes to raw lookup performance in all tests. We can also mention that the \u0026ldquo;tighter\u0026rdquo; the codepoints to lookup are, the more comparable the both techniques are. We could also plot bytes-per-codepoint vs lookup perf.\nIn this chart we can clearly see each approach \u0026ldquo;banding\u0026rdquo; on bytes-per-codepoint and lookup-perf.\nWe\u0026rsquo;ll end with mentioning performance in an non-optimized build as well, something that I find usually is lacking. OK performance in a debug-build is really something that will make your life easier and something I find to be a well worthy goal to strive for.\nIt\u0026rsquo;s the same pattern here, the std::-based solutions are just getting crushed and the simple array is by far the fastest but IMHO utf8_lookup holds its ground pretty well. A colleague of mine also pointed out that this is with gcc:s implementation of the STL, if this would have been run with some other STL implementations ( among others the one used in msvc ) the debug-results would have been even worse.\nConclusion It has been quite interesting to test and benchmark this solution. I guess the findings can be summarized as follows, if you want pure performance nothing beats a simple array ( nothing new under the sun here! ), however utf8_lookup performs really well when it comes to memory-usage.\nI also think that there might be other approaches worth testing and adding benchmarks for here and maybe some more investigations into what governs performance. My guess is cache, i.e. depending on how the indexed codepoints are distributed it could give better or worse cache-utilization.\nThere might also be gains to be had in the actual utf8_lookup implementation, for example one might change the order of how the internal items are stored to better group used memory chunks depending on access-patters. It might be interesting to generate some \u0026ldquo;heatmaps\u0026rdquo; of used codepoints for the different texts and see if a patter emerges.\nHowever as I am, depending on when your read this, the father of 2 this is all that I have the time and energy for now and it is good enough for the small things I use it for.\nI hope that this might be useful for someone, maybe it can be used for something other than what I have used it for? And if there is something to really take away from this is that base_offset + (popcnt(bits_before)) is a really sweet technique that can be used for many things :)\nHave I missed any benchmark approach? Any improvements that I could do? Anything else? Feel free to contact me on twitter!\n"
            }
    
        ,
            {
                "id": 15,
                "href": "https://kihlander.net/post/why-i-prefer-inline-forward-daclares-in-c/",
                "title": "Why I prefer inline forward-declares in C++",
                "section": "post",
                "date" : "2016.11.14",
                "body": "Time for a short post on how I usually do the humble forward declare in C++. I guess this is not something new but it is something I usually do not see in others code so it feels worth sharing.\nSo lets start of with just defining what we are talking about just to get everyone on the same page, we are talking about declaring only a class/struct name to the compiler and not having to provide the entire class/struct declaration. Mostly used as a compile-time optimization or to handle circular deps etc.\nA simple example:\n// forward declare this struct. struct a_forward_declared_struct; struct my_struct { a_forward_declared_struct* a_forward_declared_pointer; }; void my_func( a_forward_declared_struct* another_pointer ); We\u0026rsquo;re all used to see this kind of code, nothing new under the sun. I however like to write it like this:\nstruct my_struct { struct a_forward_declared_struct* a_forward_declared_pointer; }; void my_func( struct a_forward_declared_struct* another_pointer ); I don\u0026rsquo;t know if inline-forward-declare is the correct term, but we\u0026rsquo;ll use that until I\u0026rsquo;m told otherwise ;)\nSo what is better with this more verbose variant? Well, I have 2 reasons:\nIt do not \u0026ldquo;leak\u0026rdquo; definitions into the global namespace. When usage is removed, so is the forward declare. Lets go through them one-by-one.\nIt do not \u0026ldquo;leak\u0026rdquo; definitions into the global namespace. The big thing here is that we can\u0026rsquo;t break other code by removing our forward declares. Say that you have this code:\n\u0026lt;header1.h\u0026gt;\nstruct a_forward_declared_struct; void my_func1( a_forward_declared_struct* another_pointer ); \u0026lt;header2.h\u0026gt;\nvoid my_func2( a_forward_declared_struct* another_pointer ); // OH NOES, we forgot our forward declare! file.cpp\n#include \u0026#34;header1.h\u0026#34; #include \u0026#34;header2.h\u0026#34; #include \u0026#34;a_forward_declared_struct.h\u0026#34; // defining the actual struct. void a_function( a_forward_declared_struct* ptr ) { my_func2( ptr ); } Now we work with our code and refactor my_func1() to no longer take a pointer to a a_forward_declared_struct and removing the forward declare. By doing this we break file.cpp since \u0026ldquo;header2.h\u0026rdquo; is now \u0026ldquo;incomplete\u0026rdquo;. This might not be a big issue in a smaller code-base but in a bigger one (especially one using unity-builds, batch-builds or whatever you want to call them ) this can pop up on another colleagues machine after you have submitted your code.\nIf instead you would have used inline forward-declares header2.h would never have compiled to begin with so the initial implementer would not have missed the needed declarations.\nWhen usage is removed, so is the forward declare. The second improvement over the \u0026ldquo;ordinary\u0026rdquo; declarations is the fact that they are automatically removed when they are no longer needed since they are part of the actual code.\nHow many times haven\u0026rsquo;t we all found forward declares that no one uses ( and no one want to remove due to point 1 ;) ).\nSome thing like:\nclass vec3; struct i_have_no_vec3 { int apa; int kossa; // I had a vec3 here a few years ago! }; Final words Are there any drawbacks? Well, except for being slightly more verbose, the only one I can think of is that it do not work together with namespaces. For me that is no real problem since I really do not like namespaces to begin with ( topic for another rant/blog-post? ) but if you do this tip is not as useful. If you mix and match I would still recommend using inline forward-declares where possible and fallback to namespace:d declares when you have to.\nnamespace foo { class bar; }; Do you agree, am I totally wrong? Feel free to tell me on twitter!\n"
            }
    
        ,
            {
                "id": 16,
                "href": "https://kihlander.net/post/compile-time-hashes-in-c-im-not-convinced/",
                "title": "Compile-time hashes in c++, im not convinced!",
                "section": "post",
                "date" : "2016.09.24",
                "body": "I recently read a blogpost about compile-time string-hashes and constexpr and I\u0026rsquo;m still not convinced and see no real reason to leave my old and true friend the script :)\nSo first of lets look at the problem we want to solve. We want a way to do things like this and not pay the runtime cost ( and in this case just compile! ).\nvoid my_func( uint32_t val ) { switch( val ) { case HashOfString(\u0026#34;some_string\u0026#34;): do_some_stuff(); break; case HashOfString(\u0026#34;some_other_string\u0026#34;): do_some_other_stuff(); break; } } Simple enough. What seems to come up over and over again is ways of doing this with the compiler compile-time and now recently just marking HashOfString as constexpr and \u0026ldquo;trust the compiler\u0026rdquo;. The solution I usually fall back to is to just have a text-file where each line is hashed with a custom script and written to a .h file with values such as:\nmy_hashes.hash\nsome string some other string my_hashes.hash.h\n#pragma once #define HASH_some_string 0xABCD0123 // hash of \u0026#34;some_string\u0026#34; #define HASH_some_other_string 0x0123ABCD // hash of \u0026#34;some_other_string\u0026#34; usage in code\n#include \u0026#34;my_hashes.hash.h\u0026#34; void my_func( uint32_t val ) { switch( val ) { case HASH_some_string: do_some_stuff(); break; case HASH_some_other_string: do_some_other_stuff(); break; } } With a resonable buildsystem in place this can be automated and never be in your way. I have it setup to collect all \u0026lt;filename\u0026gt;.hash-files and output \u0026lt;filename\u0026gt;.hash.h.\nSo lets compare the different solutions and see why I prefer the one I do by just listing my perceived pros/cons.\nThe biggest pro for using the c++-compiler itself for this is to not need a custom buildstep for the hashes and that is a really fair point. No need to setup a buildsystem or manually generate the headers can really be an important point in some cases, especially when distributing code to others. Also having the hashed string where it is used is by some considered a pro, for me it is a + but a small one. But that is about where the pros stop i.m.h.o.\nOn the cons list I think the biggest 2 are that I have to trust the compiler to do the right thing and paying the cost for generating this each time I compile my code.\nLet\u0026rsquo;s start of with the first one, trusting the compiler. Sure, compilers are smart etc but are we sure that the compiler will optimize a HashOfString(\u0026quot;some_string) to a constant? If it does with your current compiler, will it with another compiler? What happens when a new version of your compiler is released? With the simple \u0026ldquo;generate a .h\u0026rdquo;-file I am quite sure that it will evaluate to a constant and I will not have to think about it.\nThe other issue with compile-time hashes in pure c++ is why pay for something all the time when you can pay for it once? I.e. if I put code in a .cpp to generate a hash by the compiler it will cost time each time I compile that file. When generating a header I pay for it once, when I change the text-file with the strings to hash.\nWe also have some other pros that are not as big, but I might just as well list them here for completeness:\neasier to find the actual value of the hash. When generating a header you just look in the header, when doing it with the compiler\u0026hellip; it gets harder! you have control over how the header is generated, you want to add registering of hash-value -\u0026gt; string? just add it! So what do you think, what pros have I missed on hashing with the c++-compiler? Why am I wrong?\n"
            }
    
        ,
            {
                "id": 17,
                "href": "https://kihlander.net/post/command-line-args-as-config/",
                "title": "The command-line as a poor mans config files",
                "section": "post",
                "date" : "2016.03.01",
                "body": "I like command-line arguments as mentioned in an earlier post about them. In this post I\u0026rsquo;ll discuss a method to use them as simple config-files.\nLet\u0026rsquo;s start of with a usage example from my own code. I have a meshviewer/particleviewer that is used for, you guessed it, viewing meshes and particle-effects. These kind of resources, at least the particle-effects, have internal paths to resources that need to be read while loading ( particles have a material to be rendered with etc ), i.e. resources from \u0026ldquo;some game\u0026rdquo; need to be found by the particle-viewer. Since reading resources is done via a VFS ( Virtual File System ) and paths is always specified via this VFS in resources we must just make sure that \u0026ldquo;some game\u0026rdquo;:s resources is mounted in the particle-viewer!\nLuckily for me this can be done via, you guessed it, the command line!\n./meshviewer --vfs-mount-uri=file:///path/to/assets --vfs-mount-point=/assets/ /assets/mesh/mesh_in_game_to_view.mesh Nice! But writing out this when you want to just test a resource from one project might be hard to remember and a bit of a hassle =/ So lets add one more command-line switch, --cmd-file=\u0026lt;path_to_file\u0026gt;! What this simply does is read the pointed to file, split it at white-space, add it to argc/argv. TADA! simple config-files done + all that can be configurate via files can also be configurated via the command-line.\nIf we let --cmd-file=\u0026lt;path_to_file\u0026gt; be recursive, we can do sub-files as well.\nThe above then becomes:\n./meshviewer --cmd-file=setup_some_game.cmd /assets/mesh/mesh_in_game_to_view.mesh In this specific case it might not save you that much, but consider you having multiple games, multiple configs etc.\nDo I think this would replace all configuration ever? Absolutely not, but it works great for small things as above. I would absolutely not do this for settings that should be used in a shipped game, only for debug-settings and other settings used during development.\nShort post, but hopefully someone like this and steal it :)\n"
            }
    
        ,
            {
                "id": 18,
                "href": "https://kihlander.net/post/registering-command-line-args/",
                "title": "Registering command line arguments",
                "section": "post",
                "date" : "2016.02.27",
                "body": "I really like using command line arguments. I think that it is a flexible way to interact with and configure my games/engine. It is for example easier to just add a --log-verbose=resource to set all logging in the \u0026ldquo;resource\u0026rdquo;-domain to verbose or --memory-enable-stacktrace=render to enable save of stacktraces for all allocations done in the \u0026ldquo;render\u0026rdquo;-allocator than to edit some config-file somewhere. At least for things such as the ones mentioned above, that is only set once in a while.\nNote: It\u0026rsquo;s also a simple replacement for config-files, but that is something for a later blog-post ;)\nHowever it seems like there\u0026rsquo;s always one problem, how to register supported command-line arguments to show \u0026ndash;help and check that arguments are correctly specified? In this blog-post I\u0026rsquo;ll outline a solution that I have found works really well for me. It has its drawbacks but that is usually the case with any solution to any problem ;)\nWhat do I want to achieve? Let\u0026rsquo;s make a quick list over what I want from my system.\nDifferent systems to be able to register their supported command line arguments in a simple fashion. Automatic \u0026ndash;help generation ( I always forget what flags are there etc, \u0026ndash;help to the rescue ) Systems that register args should be able to assume that all flags are valid when they get the args. How I do it First of all I let all systems parse their own argc/argv, in other words I just pass each system a reference to argc/argv. This is done in different ways, but usually something like this:\nlog_ctx_t logger = log_ctx_create( /*... some param ... */, argc, argv ); or this:\nrenderer_create_info create_info; // ... other params ... create_info.argc = argc; create_info.argv = argv; renderer_t r = renderer_create(\u0026amp;create_info); The systems get access to a const argc/argv and its their job to parse them by them self. For this I use ( shameless self-promotion comming up ) this getopt-parser https://github.com/wc-duck/getopt. But how does that tie in with our earlier demands on the \u0026ldquo;system\u0026rdquo;. Well, lets use some thing that some one consider the c++-equivalent of swearing in church, global constructors! Lets introduce a simple helper-class and macro GETOPT_ARGS_REGISTER().\nstruct __getopt_args_register { __getopt_args_register( const char* t, const getopt_option_t* opt ) : next( first ) , options_title( t ) , opts( opt ) { first = this; } static __getopt_args_register* first; __getopt_args_register* next; const char* options_title; const getopt_option_t* opts; }; #define GETOPT_ARGS_REGISTER( options_title, options ) \\ static __getopt_args_register JOIN_MACRO_TOKENS( __getopt_reg, __LINE__ )( options_title, options ) And we use this as follows\nstatic getopt_option_t options_list[] = { { \u0026#34;log-info\u0026#34;, 0x0, GETOPT_OPTION_TYPE_OPTIONAL, 0x0, \u0026#39;i\u0026#39;, \u0026#34;set log-level info, globally if no domain is specified.\u0026#34;, \u0026#34;DOMAIN\u0026#34; }, { \u0026#34;log-error\u0026#34;, 0x0, GETOPT_OPTION_TYPE_OPTIONAL, 0x0, \u0026#39;e\u0026#39;, \u0026#34;set log-level error, globally if no domain is specified.\u0026#34;, \u0026#34;DOMAIN\u0026#34; }, { \u0026#34;log-warning\u0026#34;, 0x0, GETOPT_OPTION_TYPE_OPTIONAL, 0x0, \u0026#39;w\u0026#39;, \u0026#34;set log-level warning, globally if no domain is specified\u0026#34;, \u0026#34;DOMAIN\u0026#34; }, { \u0026#34;log-verbose\u0026#34;, \u0026#39;v\u0026#39;, GETOPT_OPTION_TYPE_OPTIONAL, 0x0, \u0026#39;v\u0026#39;, \u0026#34;set log-level verbose, globally if no domain is specified\u0026#34;, \u0026#34;DOMAIN\u0026#34; }, { \u0026#34;log-callstack\u0026#34;, 0x0, GETOPT_OPTION_TYPE_OPTIONAL, 0x0, \u0026#39;c\u0026#39;, \u0026#34;log callstacks together with messages, globally if no domain is specified\u0026#34;, \u0026#34;DOMAIN\u0026#34; }, { \u0026#34;log-domains\u0026#34;, 0x0, GETOPT_OPTION_TYPE_NO_ARG, 0x0, \u0026#39;d\u0026#39;, \u0026#34;log all available domains as they are discovered.\u0026#34; }, GETOPT_OPTIONS_END }; GETOPT_ARGS_REGISTER( \u0026#34;log\u0026#34;, options_list ); What the above macro basically does is on old trick, building an global linked list of __getopt_args_register when running global constructors that can be accessed via __getopt_args_register::first.\nWhen we have this info it is an easy thing to just loop over all registered args and do the error checking and \u0026ndash;help generation etc without the systems having to know about it. I usually do this as a really early part of int main( int argc, const char** argv ). Also notice that the registered options is the same type that is used by the getopt-library so that the same setup can be used during arg-parse. Keeping the registered args defined in one place and one place only.\nOne of the things I like most with this is that the registering is done link-time, so linking to a static library, in my case render, debug or vfs ( to mention a few ) auto-registers its options. So if a lib is not used/linked, no options is registered.\nDrawbacks Well there are some of course. This will not work well together will DLL:s since the main .exe and the .dll:s will get their own instance of __getopt_args_register::first and the ones in the dll will not be accessible from the exe. It could be solved by \u0026ldquo;pulling out\u0026rdquo; __getopt_args_register::first from each DLL and manually \u0026ldquo;link\u0026rdquo; them together but that is not something I have done or have had any need for.\nAlso there is the problem of colliding flag-names and that is best solved by just not sharing flag-names, I prefix my flags by system. In some cases you might even want colliding flag-names where you have flags that should be used by multiple systems. Not sure if it is a good idea, but it is definitely something that can be done.\nConclusion This is a technique that has served me well for this purpose and the \u0026ldquo;global linked list created with global constructors\u0026rdquo; could be a useful tool in your toolbox when writing c++. It need to be used restrictively, but at least for this purpose it has not been any problems for me.\n"
            }
    
]
